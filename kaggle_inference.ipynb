{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-15T18:47:47.490427Z","iopub.status.busy":"2023-10-15T18:47:47.490081Z","iopub.status.idle":"2023-10-15T18:47:47.496138Z","shell.execute_reply":"2023-10-15T18:47:47.495166Z","shell.execute_reply.started":"2023-10-15T18:47:47.490401Z"},"papermill":{"duration":0.675684,"end_time":"2023-10-11T16:14:56.174347","exception":false,"start_time":"2023-10-11T16:14:55.498663","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import json\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:47.499417Z","iopub.status.busy":"2023-10-15T18:47:47.498183Z","iopub.status.idle":"2023-10-15T18:47:47.520381Z","shell.execute_reply":"2023-10-15T18:47:47.518387Z","shell.execute_reply.started":"2023-10-15T18:47:47.499381Z"},"papermill":{"duration":0.022356,"end_time":"2023-10-11T16:14:56.203180","exception":false,"start_time":"2023-10-11T16:14:56.180824","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["columns = pd.read_csv(\"/kaggle/input/rsna-2023-abdominal-trauma-detection/sample_submission.csv\").columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:47.524631Z","iopub.status.busy":"2023-10-15T18:47:47.524379Z","iopub.status.idle":"2023-10-15T18:47:48.593710Z","shell.execute_reply":"2023-10-15T18:47:48.592377Z","shell.execute_reply.started":"2023-10-15T18:47:47.524609Z"},"papermill":{"duration":0.962084,"end_time":"2023-10-11T16:14:57.171409","exception":false,"start_time":"2023-10-11T16:14:56.209325","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!ls ../input/openmmlab-offline-installation-mmcv-full/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:48.597033Z","iopub.status.busy":"2023-10-15T18:47:48.596601Z","iopub.status.idle":"2023-10-15T18:47:50.498151Z","shell.execute_reply":"2023-10-15T18:47:50.497006Z","shell.execute_reply.started":"2023-10-15T18:47:48.596988Z"},"papermill":{"duration":2.599297,"end_time":"2023-10-11T16:14:59.777159","exception":false,"start_time":"2023-10-11T16:14:57.177862","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install /kaggle/input/mmcv161py3/mmcv-1.6.1-py2.py3-none-any.whl -f ./ --no-index --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:50.500659Z","iopub.status.busy":"2023-10-15T18:47:50.500277Z","iopub.status.idle":"2023-10-15T18:47:52.436252Z","shell.execute_reply":"2023-10-15T18:47:52.434973Z","shell.execute_reply.started":"2023-10-15T18:47:50.500626Z"},"papermill":{"duration":3.303634,"end_time":"2023-10-11T16:15:03.094142","exception":false,"start_time":"2023-10-11T16:14:59.790508","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install /kaggle/input/pydicom-offline-installer/pydicom-2.3.1-py3-none-any.whl -f ./ --no-index --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:52.440369Z","iopub.status.busy":"2023-10-15T18:47:52.439966Z","iopub.status.idle":"2023-10-15T18:47:54.513230Z","shell.execute_reply":"2023-10-15T18:47:54.512111Z","shell.execute_reply.started":"2023-10-15T18:47:52.440341Z"},"papermill":{"duration":4.952039,"end_time":"2023-10-11T16:15:08.053058","exception":false,"start_time":"2023-10-11T16:15:03.101019","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install /kaggle/input/simpleitk-offline-installation-wheel/SimpleITK-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f ./ --no-index --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:54.515657Z","iopub.status.busy":"2023-10-15T18:47:54.515283Z","iopub.status.idle":"2023-10-15T18:47:56.400197Z","shell.execute_reply":"2023-10-15T18:47:56.398998Z","shell.execute_reply.started":"2023-10-15T18:47:54.515623Z"},"papermill":{"duration":2.491289,"end_time":"2023-10-11T16:15:10.558996","exception":false,"start_time":"2023-10-11T16:15:08.067707","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install /kaggle/input/simpleitk-offline-installation-wheel/addict-2.4.0-py3-none-any.whl -f ./ --no-index --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:56.402769Z","iopub.status.busy":"2023-10-15T18:47:56.401957Z","iopub.status.idle":"2023-10-15T18:47:57.597930Z","shell.execute_reply":"2023-10-15T18:47:57.596645Z","shell.execute_reply.started":"2023-10-15T18:47:56.402731Z"},"papermill":{"duration":1.553996,"end_time":"2023-10-11T16:15:12.120054","exception":false,"start_time":"2023-10-11T16:15:10.566058","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!cp -r /kaggle/input/yolov8/ultralytics /kaggle/working/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.600688Z","iopub.status.busy":"2023-10-15T18:47:57.600299Z","iopub.status.idle":"2023-10-15T18:47:57.607584Z","shell.execute_reply":"2023-10-15T18:47:57.606727Z","shell.execute_reply.started":"2023-10-15T18:47:57.600653Z"},"papermill":{"duration":21.118435,"end_time":"2023-10-11T16:15:33.245650","exception":false,"start_time":"2023-10-11T16:15:12.127215","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pydicom\n","import cv2\n","from tqdm.notebook import tqdm\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from typing import Tuple, List\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","import SimpleITK as sitk\n","# from datasets.seg_dataset import DatasetSegTest\n","# from aug.seg_aug import SameResAugsAlbu\n","# from losses.seg_loss import SegFastLossCalculator\n","# from metrics.seg_metrics import multiclass_dice_coefficient\n","import glob\n","\n","from ultralytics import YOLO\n","import joblib\n","\n","from imblearn.ensemble import BalancedRandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.610351Z","iopub.status.busy":"2023-10-15T18:47:57.609698Z","iopub.status.idle":"2023-10-15T18:47:57.692985Z","shell.execute_reply":"2023-10-15T18:47:57.691780Z","shell.execute_reply.started":"2023-10-15T18:47:57.610316Z"},"papermill":{"duration":2.534554,"end_time":"2023-10-11T16:15:35.787381","exception":false,"start_time":"2023-10-11T16:15:33.252827","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import logging\n","import warnings\n","\n","import torch\n","from mmcv.cnn import build_activation_layer, ConvModule, NonLocal3d#, kaiming_init, constant_init\n","#from mmcv.runner import _load_checkpoint, load_checkpoint\n","from torch import nn\n","from torch.nn.modules.batchnorm import _BatchNorm\n","from torch.nn.modules.utils import _ntuple, _triple\n","\n","\n","class BasicBlock3d(nn.Module):\n","    \"\"\"BasicBlock 3d block for ResNet3D.\n","    Args:\n","        inplanes (int): Number of channels for the input in first conv3d layer.\n","        planes (int): Number of channels produced by some norm/conv3d layers.\n","        spatial_stride (int): Spatial stride in the conv3d layer. Default: 1.\n","        temporal_stride (int): Temporal stride in the conv3d layer. Default: 1.\n","        dilation (int): Spacing between kernel elements. Default: 1.\n","        downsample (nn.Module | None): Downsample layer. Default: None.\n","        style (str): ``pytorch`` or ``caffe``. If set to \"pytorch\", the\n","            stride-two layer is the 3x3 conv layer, otherwise the stride-two\n","            layer is the first 1x1 conv layer. Default: 'pytorch'.\n","        inflate (bool): Whether to inflate kernel. Default: True.\n","        non_local (bool): Determine whether to apply non-local module in this\n","            block. Default: False.\n","        non_local_cfg (dict): Config for non-local module. Default: ``dict()``.\n","        conv_cfg (dict): Config dict for convolution layer.\n","            Default: ``dict(type='Conv3d')``.\n","        norm_cfg (dict): Config for norm layers. required keys are ``type``,\n","            Default: ``dict(type='BN3d')``.\n","        act_cfg (dict): Config dict for activation layer.\n","            Default: ``dict(type='ReLU')``.\n","        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n","            memory while slowing down the training speed. Default: False.\n","    \"\"\"\n","    expansion = 1\n","\n","    def __init__(self,\n","                 inplanes,\n","                 planes,\n","                 spatial_stride=1,\n","                 temporal_stride=1,\n","                 dilation=1,\n","                 downsample=None,\n","                 style='pytorch',\n","                 inflate=True,\n","                 non_local=False,\n","                 non_local_cfg=dict(),\n","                 conv_cfg=dict(type='Conv3d'),\n","                 norm_cfg=dict(type='BN3d'),\n","                 act_cfg=dict(type='ReLU'),\n","                 with_cp=False,\n","                 **kwargs):\n","        super().__init__()\n","        assert style in ['pytorch', 'caffe']\n","        # make sure that only ``inflate_style`` is passed into kwargs\n","        assert set(kwargs).issubset(['inflate_style'])\n","\n","        self.inplanes = inplanes\n","        self.planes = planes\n","        self.spatial_stride = spatial_stride\n","        self.temporal_stride = temporal_stride\n","        self.dilation = dilation\n","        self.style = style\n","        self.inflate = inflate\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        self.with_cp = with_cp\n","        self.non_local = non_local\n","        self.non_local_cfg = non_local_cfg\n","\n","        self.conv1_stride_s = spatial_stride\n","        self.conv2_stride_s = 1\n","        self.conv1_stride_t = temporal_stride\n","        self.conv2_stride_t = 1\n","\n","        if self.inflate:\n","            conv1_kernel_size = (3, 3, 3)\n","            conv1_padding = (1, dilation, dilation)\n","            conv2_kernel_size = (3, 3, 3)\n","            conv2_padding = (1, 1, 1)\n","        else:\n","            conv1_kernel_size = (1, 3, 3)\n","            conv1_padding = (0, dilation, dilation)\n","            conv2_kernel_size = (1, 3, 3)\n","            conv2_padding = (0, 1, 1)\n","\n","        self.conv1 = ConvModule(\n","            inplanes,\n","            planes,\n","            conv1_kernel_size,\n","            stride=(self.conv1_stride_t, self.conv1_stride_s,\n","                    self.conv1_stride_s),\n","            padding=conv1_padding,\n","            dilation=(1, dilation, dilation),\n","            bias=False,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","        self.conv2 = ConvModule(\n","            planes,\n","            planes * self.expansion,\n","            conv2_kernel_size,\n","            stride=(self.conv2_stride_t, self.conv2_stride_s,\n","                    self.conv2_stride_s),\n","            padding=conv2_padding,\n","            bias=False,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=None)\n","\n","        self.downsample = downsample\n","        self.relu = build_activation_layer(self.act_cfg)\n","\n","        if self.non_local:\n","            self.non_local_block = NonLocal3d(self.conv2.norm.num_features,\n","                                              **self.non_local_cfg)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the computation performed at every call.\"\"\"\n","\n","        def _inner_forward(x):\n","            \"\"\"Forward wrapper for utilizing checkpoint.\"\"\"\n","            identity = x\n","\n","            out = self.conv1(x)\n","            out = self.conv2(out)\n","\n","            if self.downsample is not None:\n","                identity = self.downsample(x)\n","\n","            out = out + identity\n","            return out\n","\n","        if self.with_cp and x.requires_grad:\n","            out = cp.checkpoint(_inner_forward, x)\n","        else:\n","            out = _inner_forward(x)\n","        out = self.relu(out)\n","\n","        if self.non_local:\n","            out = self.non_local_block(out)\n","\n","        return out\n","\n","\n","class Bottleneck3d(nn.Module):\n","    \"\"\"Bottleneck 3d block for ResNet3D.\n","    Args:\n","        inplanes (int): Number of channels for the input in first conv3d layer.\n","        planes (int): Number of channels produced by some norm/conv3d layers.\n","        spatial_stride (int): Spatial stride in the conv3d layer. Default: 1.\n","        temporal_stride (int): Temporal stride in the conv3d layer. Default: 1.\n","        dilation (int): Spacing between kernel elements. Default: 1.\n","        downsample (nn.Module | None): Downsample layer. Default: None.\n","        style (str): ``pytorch`` or ``caffe``. If set to \"pytorch\", the\n","            stride-two layer is the 3x3 conv layer, otherwise the stride-two\n","            layer is the first 1x1 conv layer. Default: 'pytorch'.\n","        inflate (bool): Whether to inflate kernel. Default: True.\n","        inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines the\n","            kernel sizes and padding strides for conv1 and conv2 in each block.\n","            Default: '3x1x1'.\n","        non_local (bool): Determine whether to apply non-local module in this\n","            block. Default: False.\n","        non_local_cfg (dict): Config for non-local module. Default: ``dict()``.\n","        conv_cfg (dict): Config dict for convolution layer.\n","            Default: ``dict(type='Conv3d')``.\n","        norm_cfg (dict): Config for norm layers. required keys are ``type``,\n","            Default: ``dict(type='BN3d')``.\n","        act_cfg (dict): Config dict for activation layer.\n","            Default: ``dict(type='ReLU')``.\n","        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n","            memory while slowing down the training speed. Default: False.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self,\n","                 inplanes,\n","                 planes,\n","                 spatial_stride=1,\n","                 temporal_stride=1,\n","                 dilation=1,\n","                 downsample=None,\n","                 style='pytorch',\n","                 inflate=True,\n","                 inflate_style='3x1x1',\n","                 non_local=False,\n","                 non_local_cfg=dict(),\n","                 conv_cfg=dict(type='Conv3d'),\n","                 norm_cfg=dict(type='BN3d'),\n","                 act_cfg=dict(type='ReLU'),\n","                 with_cp=False):\n","        super().__init__()\n","        assert style in ['pytorch', 'caffe']\n","        assert inflate_style in ['3x1x1', '3x3x3']\n","\n","        self.inplanes = inplanes\n","        self.planes = planes\n","        self.spatial_stride = spatial_stride\n","        self.temporal_stride = temporal_stride\n","        self.dilation = dilation\n","        self.style = style\n","        self.inflate = inflate\n","        self.inflate_style = inflate_style\n","        self.norm_cfg = norm_cfg\n","        self.conv_cfg = conv_cfg\n","        self.act_cfg = act_cfg\n","        self.with_cp = with_cp\n","        self.non_local = non_local\n","        self.non_local_cfg = non_local_cfg\n","\n","        if self.style == 'pytorch':\n","            self.conv1_stride_s = 1\n","            self.conv2_stride_s = spatial_stride\n","            self.conv1_stride_t = 1\n","            self.conv2_stride_t = temporal_stride\n","        else:\n","            self.conv1_stride_s = spatial_stride\n","            self.conv2_stride_s = 1\n","            self.conv1_stride_t = temporal_stride\n","            self.conv2_stride_t = 1\n","\n","        if self.inflate:\n","            if inflate_style == '3x1x1':\n","                conv1_kernel_size = (3, 1, 1)\n","                conv1_padding = (1, 0, 0)\n","                conv2_kernel_size = (1, 3, 3)\n","                conv2_padding = (0, dilation, dilation)\n","            else:\n","                conv1_kernel_size = (1, 1, 1)\n","                conv1_padding = (0, 0, 0)\n","                conv2_kernel_size = (3, 3, 3)\n","                conv2_padding = (1, dilation, dilation)\n","        else:\n","            conv1_kernel_size = (1, 1, 1)\n","            conv1_padding = (0, 0, 0)\n","            conv2_kernel_size = (1, 3, 3)\n","            conv2_padding = (0, dilation, dilation)\n","\n","        self.conv1 = ConvModule(\n","            inplanes,\n","            planes,\n","            conv1_kernel_size,\n","            stride=(self.conv1_stride_t, self.conv1_stride_s,\n","                    self.conv1_stride_s),\n","            padding=conv1_padding,\n","            bias=False,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","        self.conv2 = ConvModule(\n","            planes,\n","            planes,\n","            conv2_kernel_size,\n","            stride=(self.conv2_stride_t, self.conv2_stride_s,\n","                    self.conv2_stride_s),\n","            padding=conv2_padding,\n","            dilation=(1, dilation, dilation),\n","            bias=False,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","        self.conv3 = ConvModule(\n","            planes,\n","            planes * self.expansion,\n","            1,\n","            bias=False,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            # No activation in the third ConvModule for bottleneck\n","            act_cfg=None)\n","\n","        self.downsample = downsample\n","        self.relu = build_activation_layer(self.act_cfg)\n","\n","        if self.non_local:\n","            self.non_local_block = NonLocal3d(self.conv3.norm.num_features,\n","                                              **self.non_local_cfg)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the computation performed at every call.\"\"\"\n","\n","        def _inner_forward(x):\n","            \"\"\"Forward wrapper for utilizing checkpoint.\"\"\"\n","            identity = x\n","\n","            out = self.conv1(x)\n","            out = self.conv2(out)\n","            out = self.conv3(out)\n","\n","            if self.downsample is not None:\n","                identity = self.downsample(x)\n","\n","            out = out + identity\n","            return out\n","\n","        if self.with_cp and x.requires_grad:\n","            out = cp.checkpoint(_inner_forward, x)\n","        else:\n","            out = _inner_forward(x)\n","        out = self.relu(out)\n","\n","        if self.non_local:\n","            out = self.non_local_block(out)\n","\n","        return out\n","\n","\n","class ResNet3d(nn.Module):\n","    \"\"\"ResNet 3d backbone.\n","    Args:\n","        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n","        pretrained (str | None): Name of pretrained model.\n","        stage_blocks (tuple | None): Set number of stages for each res layer.\n","            Default: None.\n","        pretrained2d (bool): Whether to load pretrained 2D model.\n","            Default: True.\n","        in_channels (int): Channel num of input features. Default: 3.\n","        base_channels (int): Channel num of stem output features. Default: 64.\n","        out_indices (Sequence[int]): Indices of output feature. Default: (3, ).\n","        num_stages (int): Resnet stages. Default: 4.\n","        spatial_strides (Sequence[int]):\n","            Spatial strides of residual blocks of each stage.\n","            Default: ``(1, 2, 2, 2)``.\n","        temporal_strides (Sequence[int]):\n","            Temporal strides of residual blocks of each stage.\n","            Default: ``(1, 1, 1, 1)``.\n","        dilations (Sequence[int]): Dilation of each stage.\n","            Default: ``(1, 1, 1, 1)``.\n","        conv1_kernel (Sequence[int]): Kernel size of the first conv layer.\n","            Default: ``(3, 7, 7)``.\n","        conv1_stride_s (int): Spatial stride of the first conv layer.\n","            Default: 2.\n","        conv1_stride_t (int): Temporal stride of the first conv layer.\n","            Default: 1.\n","        pool1_stride_s (int): Spatial stride of the first pooling layer.\n","            Default: 2.\n","        pool1_stride_t (int): Temporal stride of the first pooling layer.\n","            Default: 1.\n","        with_pool2 (bool): Whether to use pool2. Default: True.\n","        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n","            layer is the 3x3 conv layer, otherwise the stride-two layer is\n","            the first 1x1 conv layer. Default: 'pytorch'.\n","        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n","            not freezing any parameters. Default: -1.\n","        inflate (Sequence[int]): Inflate Dims of each block.\n","            Default: (1, 1, 1, 1).\n","        inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines the\n","            kernel sizes and padding strides for conv1 and conv2 in each block.\n","            Default: '3x1x1'.\n","        conv_cfg (dict): Config for conv layers. required keys are ``type``\n","            Default: ``dict(type='Conv3d')``.\n","        norm_cfg (dict): Config for norm layers. required keys are ``type`` and\n","            ``requires_grad``.\n","            Default: ``dict(type='BN3d', requires_grad=True)``.\n","        act_cfg (dict): Config dict for activation layer.\n","            Default: ``dict(type='ReLU', inplace=True)``.\n","        norm_eval (bool): Whether to set BN layers to eval mode, namely, freeze\n","            running stats (mean and var). Default: False.\n","        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n","            memory while slowing down the training speed. Default: False.\n","        non_local (Sequence[int]): Determine whether to apply non-local module\n","            in the corresponding block of each stages. Default: (0, 0, 0, 0).\n","        non_local_cfg (dict): Config for non-local module. Default: ``dict()``.\n","        zero_init_residual (bool):\n","            Whether to use zero initialization for residual block,\n","            Default: True.\n","        kwargs (dict, optional): Key arguments for \"make_res_layer\".\n","    \"\"\"\n","\n","    arch_settings = {\n","        18: (BasicBlock3d, (2, 2, 2, 2)),\n","        34: (BasicBlock3d, (3, 4, 6, 3)),\n","        50: (Bottleneck3d, (3, 4, 6, 3)),\n","        101: (Bottleneck3d, (3, 4, 23, 3)),\n","        152: (Bottleneck3d, (3, 8, 36, 3))\n","    }\n","\n","    def __init__(self,\n","                 depth,\n","                 pretrained,\n","                 stage_blocks=None,\n","                 pretrained2d=True,\n","                 in_channels=3,\n","                 num_stages=4,\n","                 base_channels=64,\n","                 out_indices=(0, 1, 2, 3,),\n","                 spatial_strides=(1, 2, 2, 2),\n","                 temporal_strides=(1, 1, 1, 1),\n","                 dilations=(1, 1, 1, 1),\n","                 conv1_kernel=(3, 7, 7),\n","                 conv1_stride_s=2,\n","                 conv1_stride_t=1,\n","                 pool1_stride_s=2,\n","                 pool1_stride_t=1,\n","                 with_pool1=True,\n","                 with_pool2=True,\n","                 style='pytorch',\n","                 frozen_stages=-1,\n","                 inflate=(1, 1, 1, 1),\n","                 inflate_style='3x1x1',\n","                 conv_cfg=dict(type='Conv3d'),\n","                 norm_cfg=dict(type='BN3d', requires_grad=True),\n","                 act_cfg=dict(type='ReLU', inplace=True),\n","                 norm_eval=False,\n","                 with_cp=False,\n","                 non_local=(0, 0, 0, 0),\n","                 non_local_cfg=dict(),\n","                 zero_init_residual=True,\n","                 **kwargs):\n","        super().__init__()\n","        if depth not in self.arch_settings:\n","            raise KeyError(f'invalid depth {depth} for resnet')\n","        self.depth = depth\n","        self.pretrained = pretrained\n","        self.pretrained2d = pretrained2d\n","        self.in_channels = in_channels\n","        self.base_channels = base_channels\n","        self.num_stages = num_stages\n","        assert 1 <= num_stages <= 4\n","        self.stage_blocks = stage_blocks\n","        self.out_indices = out_indices\n","        assert max(out_indices) < num_stages\n","        self.spatial_strides = spatial_strides\n","        self.temporal_strides = temporal_strides\n","        self.dilations = dilations\n","        assert len(spatial_strides) == len(temporal_strides) == len(\n","            dilations) == num_stages\n","        if self.stage_blocks is not None:\n","            assert len(self.stage_blocks) == num_stages\n","\n","        self.conv1_kernel = conv1_kernel\n","        self.conv1_stride_s = conv1_stride_s\n","        self.conv1_stride_t = conv1_stride_t\n","        self.pool1_stride_s = pool1_stride_s\n","        self.pool1_stride_t = pool1_stride_t\n","        self.with_pool1 = with_pool1\n","        self.with_pool2 = with_pool2\n","        self.style = style\n","        self.frozen_stages = frozen_stages\n","        self.stage_inflations = _ntuple(num_stages)(inflate)\n","        self.non_local_stages = _ntuple(num_stages)(non_local)\n","        self.inflate_style = inflate_style\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        self.norm_eval = norm_eval\n","        self.with_cp = with_cp\n","        self.zero_init_residual = zero_init_residual\n","\n","        self.block, stage_blocks = self.arch_settings[depth]\n","\n","        if self.stage_blocks is None:\n","            self.stage_blocks = stage_blocks[:num_stages]\n","\n","        self.inplanes = self.base_channels\n","\n","        self.non_local_cfg = non_local_cfg\n","\n","        self._make_stem_layer()\n","\n","        self.res_layers = []\n","        for i, num_blocks in enumerate(self.stage_blocks):\n","            spatial_stride = spatial_strides[i]\n","            temporal_stride = temporal_strides[i]\n","            dilation = dilations[i]\n","            planes = self.base_channels * 2 ** i\n","            res_layer = self.make_res_layer(\n","                self.block,\n","                self.inplanes,\n","                planes,\n","                num_blocks,\n","                spatial_stride=spatial_stride,\n","                temporal_stride=temporal_stride,\n","                dilation=dilation,\n","                style=self.style,\n","                norm_cfg=self.norm_cfg,\n","                conv_cfg=self.conv_cfg,\n","                act_cfg=self.act_cfg,\n","                non_local=self.non_local_stages[i],\n","                non_local_cfg=self.non_local_cfg,\n","                inflate=self.stage_inflations[i],\n","                inflate_style=self.inflate_style,\n","                with_cp=with_cp,\n","                **kwargs)\n","            self.inplanes = planes * self.block.expansion\n","            layer_name = f'layer{i + 1}'\n","            self.add_module(layer_name, res_layer)\n","            self.res_layers.append(layer_name)\n","\n","        self.feat_dim = self.block.expansion * self.base_channels * 2 ** (\n","                len(self.stage_blocks) - 1)\n","\n","    @staticmethod\n","    def make_res_layer(block,\n","                       inplanes,\n","                       planes,\n","                       blocks,\n","                       spatial_stride=1,\n","                       temporal_stride=1,\n","                       dilation=1,\n","                       style='pytorch',\n","                       inflate=1,\n","                       inflate_style='3x1x1',\n","                       non_local=0,\n","                       non_local_cfg=dict(),\n","                       norm_cfg=None,\n","                       act_cfg=None,\n","                       conv_cfg=None,\n","                       with_cp=False,\n","                       **kwargs):\n","        \"\"\"Build residual layer for ResNet3D.\n","        Args:\n","            block (nn.Module): Residual module to be built.\n","            inplanes (int): Number of channels for the input feature\n","                in each block.\n","            planes (int): Number of channels for the output feature\n","                in each block.\n","            blocks (int): Number of residual blocks.\n","            spatial_stride (int | Sequence[int]): Spatial strides in\n","                residual and conv layers. Default: 1.\n","            temporal_stride (int | Sequence[int]): Temporal strides in\n","                residual and conv layers. Default: 1.\n","            dilation (int): Spacing between kernel elements. Default: 1.\n","            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,\n","                the stride-two layer is the 3x3 conv layer, otherwise\n","                the stride-two layer is the first 1x1 conv layer.\n","                Default: ``pytorch``.\n","            inflate (int | Sequence[int]): Determine whether to inflate\n","                for each block. Default: 1.\n","            inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines\n","                the kernel sizes and padding strides for conv1 and conv2\n","                in each block. Default: '3x1x1'.\n","            non_local (int | Sequence[int]): Determine whether to apply\n","                non-local module in the corresponding block of each stages.\n","                Default: 0.\n","            non_local_cfg (dict): Config for non-local module.\n","                Default: ``dict()``.\n","            conv_cfg (dict | None): Config for norm layers. Default: None.\n","            norm_cfg (dict | None): Config for norm layers. Default: None.\n","            act_cfg (dict | None): Config for activate layers. Default: None.\n","            with_cp (bool | None): Use checkpoint or not. Using checkpoint\n","                will save some memory while slowing down the training speed.\n","                Default: False.\n","        Returns:\n","            nn.Module: A residual layer for the given config.\n","        \"\"\"\n","        inflate = inflate if not isinstance(inflate,\n","                                            int) else (inflate,) * blocks\n","        non_local = non_local if not isinstance(\n","            non_local, int) else (non_local,) * blocks\n","        assert len(inflate) == blocks and len(non_local) == blocks\n","        downsample = None\n","        if spatial_stride != 1 or inplanes != planes * block.expansion:\n","            downsample = ConvModule(\n","                inplanes,\n","                planes * block.expansion,\n","                kernel_size=1,\n","                stride=(temporal_stride, spatial_stride, spatial_stride),\n","                bias=False,\n","                conv_cfg=conv_cfg,\n","                norm_cfg=norm_cfg,\n","                act_cfg=None)\n","\n","        layers = []\n","        layers.append(\n","            block(\n","                inplanes,\n","                planes,\n","                spatial_stride=spatial_stride,\n","                temporal_stride=temporal_stride,\n","                dilation=dilation,\n","                downsample=downsample,\n","                style=style,\n","                inflate=(inflate[0] == 1),\n","                inflate_style=inflate_style,\n","                non_local=(non_local[0] == 1),\n","                non_local_cfg=non_local_cfg,\n","                norm_cfg=norm_cfg,\n","                conv_cfg=conv_cfg,\n","                act_cfg=act_cfg,\n","                with_cp=with_cp,\n","                **kwargs))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(\n","                block(\n","                    inplanes,\n","                    planes,\n","                    spatial_stride=1,\n","                    temporal_stride=1,\n","                    dilation=dilation,\n","                    style=style,\n","                    inflate=(inflate[i] == 1),\n","                    inflate_style=inflate_style,\n","                    non_local=(non_local[i] == 1),\n","                    non_local_cfg=non_local_cfg,\n","                    norm_cfg=norm_cfg,\n","                    conv_cfg=conv_cfg,\n","                    act_cfg=act_cfg,\n","                    with_cp=with_cp,\n","                    **kwargs))\n","\n","        return nn.Sequential(*layers)\n","\n","    @staticmethod\n","    def _inflate_conv_params(conv3d, state_dict_2d, module_name_2d,\n","                             inflated_param_names):\n","        \"\"\"Inflate a conv module from 2d to 3d.\n","        Args:\n","            conv3d (nn.Module): The destination conv3d module.\n","            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n","            module_name_2d (str): The name of corresponding conv module in the\n","                2d model.\n","            inflated_param_names (list[str]): List of parameters that have been\n","                inflated.\n","        \"\"\"\n","        weight_2d_name = module_name_2d + '.weight'\n","\n","        conv2d_weight = state_dict_2d[weight_2d_name]\n","        kernel_t = conv3d.weight.data.shape[2]\n","\n","        new_weight = conv2d_weight.data.unsqueeze(2).expand_as(\n","            conv3d.weight) / kernel_t\n","        conv3d.weight.data.copy_(new_weight)\n","        inflated_param_names.append(weight_2d_name)\n","\n","        if getattr(conv3d, 'bias') is not None:\n","            bias_2d_name = module_name_2d + '.bias'\n","            conv3d.bias.data.copy_(state_dict_2d[bias_2d_name])\n","            inflated_param_names.append(bias_2d_name)\n","\n","    @staticmethod\n","    def _inflate_bn_params(bn3d, state_dict_2d, module_name_2d,\n","                           inflated_param_names):\n","        \"\"\"Inflate a norm module from 2d to 3d.\n","        Args:\n","            bn3d (nn.Module): The destination bn3d module.\n","            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n","            module_name_2d (str): The name of corresponding bn module in the\n","                2d model.\n","            inflated_param_names (list[str]): List of parameters that have been\n","                inflated.\n","        \"\"\"\n","        for param_name, param in bn3d.named_parameters():\n","            param_2d_name = f'{module_name_2d}.{param_name}'\n","            param_2d = state_dict_2d[param_2d_name]\n","            if param.data.shape != param_2d.shape:\n","                warnings.warn(f'The parameter of {module_name_2d} is not'\n","                              'loaded due to incompatible shapes. ')\n","                return\n","\n","            param.data.copy_(param_2d)\n","            inflated_param_names.append(param_2d_name)\n","\n","        for param_name, param in bn3d.named_buffers():\n","            param_2d_name = f'{module_name_2d}.{param_name}'\n","            # some buffers like num_batches_tracked may not exist in old\n","            # checkpoints\n","            if param_2d_name in state_dict_2d:\n","                param_2d = state_dict_2d[param_2d_name]\n","                param.data.copy_(param_2d)\n","                inflated_param_names.append(param_2d_name)\n","\n","    @staticmethod\n","    def _inflate_weights(self, logger):\n","        \"\"\"Inflate the resnet2d parameters to resnet3d.\n","        The differences between resnet3d and resnet2d mainly lie in an extra\n","        axis of conv kernel. To utilize the pretrained parameters in 2d model,\n","        the weight of conv2d models should be inflated to fit in the shapes of\n","        the 3d counterpart.\n","        Args:\n","            logger (logging.Logger): The logger used to print\n","                debugging infomation.\n","        \"\"\"\n","\n","        state_dict_r2d = _load_checkpoint(self.pretrained)\n","        if 'state_dict' in state_dict_r2d:\n","            state_dict_r2d = state_dict_r2d['state_dict']\n","\n","        inflated_param_names = []\n","        for name, module in self.named_modules():\n","            if isinstance(module, ConvModule):\n","                # we use a ConvModule to wrap conv+bn+relu layers, thus the\n","                # name mapping is needed\n","                if 'downsample' in name:\n","                    # layer{X}.{Y}.downsample.conv->layer{X}.{Y}.downsample.0\n","                    original_conv_name = name + '.0'\n","                    # layer{X}.{Y}.downsample.bn->layer{X}.{Y}.downsample.1\n","                    original_bn_name = name + '.1'\n","                else:\n","                    # layer{X}.{Y}.conv{n}.conv->layer{X}.{Y}.conv{n}\n","                    original_conv_name = name\n","                    # layer{X}.{Y}.conv{n}.bn->layer{X}.{Y}.bn{n}\n","                    original_bn_name = name.replace('conv', 'bn')\n","                if original_conv_name + '.weight' not in state_dict_r2d:\n","                    logger.warning(f'Module not exist in the state_dict_r2d'\n","                                   f': {original_conv_name}')\n","                else:\n","                    shape_2d = state_dict_r2d[original_conv_name +\n","                                              '.weight'].shape\n","                    shape_3d = module.conv.weight.data.shape\n","                    if shape_2d != shape_3d[:2] + shape_3d[3:]:\n","                        logger.warning(f'Weight shape mismatch for '\n","                                       f': {original_conv_name} : '\n","                                       f'3d weight shape: {shape_3d}; '\n","                                       f'2d weight shape: {shape_2d}. ')\n","                    else:\n","                        self._inflate_conv_params(module.conv, state_dict_r2d,\n","                                                  original_conv_name,\n","                                                  inflated_param_names)\n","\n","                if original_bn_name + '.weight' not in state_dict_r2d:\n","                    logger.warning(f'Module not exist in the state_dict_r2d'\n","                                   f': {original_bn_name}')\n","                else:\n","                    self._inflate_bn_params(module.bn, state_dict_r2d,\n","                                            original_bn_name,\n","                                            inflated_param_names)\n","\n","        # check if any parameters in the 2d checkpoint are not loaded\n","        remaining_names = set(\n","            state_dict_r2d.keys()) - set(inflated_param_names)\n","        if remaining_names:\n","            logger.info(f'These parameters in the 2d checkpoint are not loaded'\n","                        f': {remaining_names}')\n","\n","    def inflate_weights(self, logger):\n","        self._inflate_weights(self, logger)\n","\n","    def _make_stem_layer(self):\n","        \"\"\"Construct the stem layers consists of a conv+norm+act module and a\n","        pooling layer.\"\"\"\n","        self.conv1 = ConvModule(\n","            self.in_channels,\n","            self.base_channels,\n","            kernel_size=self.conv1_kernel,\n","            stride=(self.conv1_stride_t, self.conv1_stride_s,\n","                    self.conv1_stride_s),\n","            padding=tuple([(k - 1) // 2 for k in _triple(self.conv1_kernel)]),\n","            bias=False,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","        self.maxpool = nn.MaxPool3d(\n","            kernel_size=(1, 3, 3),\n","            stride=(self.pool1_stride_t, self.pool1_stride_s,\n","                    self.pool1_stride_s),\n","            padding=(0, 1, 1))\n","\n","        self.pool2 = nn.MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1))\n","\n","    def _freeze_stages(self):\n","        \"\"\"Prevent all the parameters from being optimized before\n","        ``self.frozen_stages``.\"\"\"\n","        if self.frozen_stages >= 0:\n","            self.conv1.eval()\n","            for param in self.conv1.parameters():\n","                param.requires_grad = False\n","\n","        for i in range(1, self.frozen_stages + 1):\n","            m = getattr(self, f'layer{i}')\n","            m.eval()\n","            for param in m.parameters():\n","                param.requires_grad = False\n","\n","    @staticmethod\n","    def _init_weights(self, pretrained=None):\n","        \"\"\"Initiate the parameters either from existing checkpoint or from\n","        scratch.\n","        Args:\n","            pretrained (str | None): The path of the pretrained weight. Will\n","                override the original `pretrained` if set. The arg is added to\n","                be compatible with mmdet. Default: None.\n","        \"\"\"\n","        if pretrained:\n","            self.pretrained = pretrained\n","        if isinstance(self.pretrained, str):\n","            logger = logging.Logger()\n","            logger.info(f'load model from: {self.pretrained}')\n","\n","            if self.pretrained2d:\n","                # Inflate 2D model into 3D model.\n","                self.inflate_weights(logger)\n","\n","            else:\n","                # Directly load 3D model.\n","                load_checkpoint(\n","                    self, self.pretrained, strict=False, logger=logger)\n","\n","        elif self.pretrained is None:\n","            for m in self.modules():\n","                if isinstance(m, nn.Conv3d):\n","                    kaiming_init(m)\n","                elif isinstance(m, _BatchNorm):\n","                    constant_init(m, 1)\n","                    \n","            if self.zero_init_residual:\n","                for m in self.modules():\n","                    if isinstance(m, Bottleneck3d):\n","                        constant_init(m.conv3.bn, 0)\n","                    elif isinstance(m, BasicBlock3d):\n","                        constant_init(m.conv2.bn, 0)\n","        else:\n","            raise TypeError('pretrained must be a str or None')\n","\n","    def init_weights(self, pretrained=None):\n","        self._init_weights(self, pretrained)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the computation performed at every call.\n","        Args:\n","            x (torch.Tensor): The input data.\n","        Returns:\n","            torch.Tensor: The feature of the input\n","            samples extracted by the backbone.\n","        \"\"\"\n","        x = self.conv1(x)\n","        outs = [x]\n","\n","        if self.with_pool1:\n","            x = self.maxpool(x)\n","        for i, layer_name in enumerate(self.res_layers):\n","            res_layer = getattr(self, layer_name)\n","            x = res_layer(x)\n","            if i == 0 and self.with_pool2:\n","                x = self.pool2(x)\n","            if i in self.out_indices:\n","                outs.append(x)\n","        if len(outs) == 1:\n","            return outs[0]\n","\n","        return tuple(outs)\n","\n","    def train(self, mode=True):\n","        \"\"\"Set the optimization status when training.\"\"\"\n","        super().train(mode)\n","        self._freeze_stages()\n","        if mode and self.norm_eval:\n","            for m in self.modules():\n","                if isinstance(m, _BatchNorm):\n","                    m.eval()\n","\n","\n","class ResNet3dLayer(nn.Module):\n","    \"\"\"ResNet 3d Layer.\n","    Args:\n","        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n","        pretrained (str | None): Name of pretrained model.\n","        pretrained2d (bool): Whether to load pretrained 2D model.\n","            Default: True.\n","        stage (int): The index of Resnet stage. Default: 3.\n","        base_channels (int): Channel num of stem output features. Default: 64.\n","        spatial_stride (int): The 1st res block's spatial stride. Default 2.\n","        temporal_stride (int): The 1st res block's temporal stride. Default 1.\n","        dilation (int): The dilation. Default: 1.\n","        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n","            layer is the 3x3 conv layer, otherwise the stride-two layer is\n","            the first 1x1 conv layer. Default: 'pytorch'.\n","        all_frozen (bool): Frozen all modules in the layer. Default: False.\n","        inflate (int): Inflate Dims of each block. Default: 1.\n","        inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines the\n","            kernel sizes and padding strides for conv1 and conv2 in each block.\n","            Default: '3x1x1'.\n","        conv_cfg (dict): Config for conv layers. required keys are ``type``\n","            Default: ``dict(type='Conv3d')``.\n","        norm_cfg (dict): Config for norm layers. required keys are ``type`` and\n","            ``requires_grad``.\n","            Default: ``dict(type='BN3d', requires_grad=True)``.\n","        act_cfg (dict): Config dict for activation layer.\n","            Default: ``dict(type='ReLU', inplace=True)``.\n","        norm_eval (bool): Whether to set BN layers to eval mode, namely, freeze\n","            running stats (mean and var). Default: False.\n","        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n","            memory while slowing down the training speed. Default: False.\n","        zero_init_residual (bool):\n","            Whether to use zero initialization for residual block,\n","            Default: True.\n","        kwargs (dict, optional): Key arguments for \"make_res_layer\".\n","    \"\"\"\n","\n","    def __init__(self,\n","                 depth,\n","                 pretrained,\n","                 pretrained2d=True,\n","                 stage=3,\n","                 base_channels=64,\n","                 spatial_stride=2,\n","                 temporal_stride=1,\n","                 dilation=1,\n","                 style='pytorch',\n","                 all_frozen=False,\n","                 inflate=1,\n","                 inflate_style='3x1x1',\n","                 conv_cfg=dict(type='Conv3d'),\n","                 norm_cfg=dict(type='BN3d', requires_grad=True),\n","                 act_cfg=dict(type='ReLU', inplace=True),\n","                 norm_eval=False,\n","                 with_cp=False,\n","                 zero_init_residual=True,\n","                 **kwargs):\n","\n","        super().__init__()\n","        self.arch_settings = ResNet3d.arch_settings\n","        assert depth in self.arch_settings\n","\n","        self.make_res_layer = ResNet3d.make_res_layer\n","        self._inflate_conv_params = ResNet3d._inflate_conv_params\n","        self._inflate_bn_params = ResNet3d._inflate_bn_params\n","        self._inflate_weights = ResNet3d._inflate_weights\n","        self._init_weights = ResNet3d._init_weights\n","\n","        self.depth = depth\n","        self.pretrained = pretrained\n","        self.pretrained2d = pretrained2d\n","        self.stage = stage\n","        # stage index is 0 based\n","        assert 0 <= stage <= 3\n","        self.base_channels = base_channels\n","\n","        self.spatial_stride = spatial_stride\n","        self.temporal_stride = temporal_stride\n","        self.dilation = dilation\n","\n","        self.style = style\n","        self.all_frozen = all_frozen\n","\n","        self.stage_inflation = inflate\n","        self.inflate_style = inflate_style\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        self.norm_eval = norm_eval\n","        self.with_cp = with_cp\n","        self.zero_init_residual = zero_init_residual\n","\n","        block, stage_blocks = self.arch_settings[depth]\n","        stage_block = stage_blocks[stage]\n","        planes = 64 * 2 ** stage\n","        inplanes = 64 * 2 ** (stage - 1) * block.expansion\n","\n","        res_layer = self.make_res_layer(\n","            block,\n","            inplanes,\n","            planes,\n","            stage_block,\n","            spatial_stride=spatial_stride,\n","            temporal_stride=temporal_stride,\n","            dilation=dilation,\n","            style=self.style,\n","            norm_cfg=self.norm_cfg,\n","            conv_cfg=self.conv_cfg,\n","            act_cfg=self.act_cfg,\n","            inflate=self.stage_inflation,\n","            inflate_style=self.inflate_style,\n","            with_cp=with_cp,\n","            **kwargs)\n","\n","        self.layer_name = f'layer{stage + 1}'\n","        self.add_module(self.layer_name, res_layer)\n","\n","    def inflate_weights(self, logger):\n","        self._inflate_weights(self, logger)\n","\n","    def _freeze_stages(self):\n","        \"\"\"Prevent all the parameters from being optimized before\n","        ``self.frozen_stages``.\"\"\"\n","        if self.all_frozen:\n","            layer = getattr(self, self.layer_name)\n","            layer.eval()\n","            for param in layer.parameters():\n","                param.requires_grad = False\n","\n","    def init_weights(self, pretrained=None):\n","        self._init_weights(self, pretrained)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the computation performed at every call.\n","        Args:\n","            x (torch.Tensor): The input data.\n","        Returns:\n","            torch.Tensor: The feature of the input\n","            samples extracted by the backbone.\n","        \"\"\"\n","        res_layer = getattr(self, self.layer_name)\n","        out = res_layer(x)\n","        return out\n","\n","    def train(self, mode=True):\n","        \"\"\"Set the optimization status when training.\"\"\"\n","        super().train(mode)\n","        self._freeze_stages()\n","        if mode and self.norm_eval:\n","            for m in self.modules():\n","                if isinstance(m, _BatchNorm):\n","                    m.eval()\n","\n","\n","class CSNBottleneck3d(Bottleneck3d):\n","    \"\"\"Channel-Separated Bottleneck Block.\n","    This module is proposed in\n","    \"Video Classification with Channel-Separated Convolutional Networks\"\n","    Link: https://arxiv.org/pdf/1711.11248.pdf\n","    Args:\n","        inplanes (int): Number of channels for the input in first conv3d layer.\n","        planes (int): Number of channels produced by some norm/conv3d layers.\n","        bottleneck_mode (str): Determine which ways to factorize a 3D\n","            bottleneck block using channel-separated convolutional networks.\n","                If set to 'ip', it will replace the 3x3x3 conv2 layer with a\n","                1x1x1 traditional convolution and a 3x3x3 depthwise\n","                convolution, i.e., Interaction-preserved channel-separated\n","                bottleneck block.\n","                If set to 'ir', it will replace the 3x3x3 conv2 layer with a\n","                3x3x3 depthwise convolution, which is derived from preserved\n","                bottleneck block by removing the extra 1x1x1 convolution,\n","                i.e., Interaction-reduced channel-separated bottleneck block.\n","            Default: 'ir'.\n","        args (position arguments): Position arguments for Bottleneck.\n","        kwargs (dict, optional): Keyword arguments for Bottleneck.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 inplanes,\n","                 planes,\n","                 *args,\n","                 bottleneck_mode='ir',\n","                 **kwargs):\n","        super(CSNBottleneck3d, self).__init__(inplanes, planes, *args,\n","                                              **kwargs)\n","        self.bottleneck_mode = bottleneck_mode\n","        conv2 = []\n","        if self.bottleneck_mode == 'ip':\n","            conv2.append(\n","                ConvModule(\n","                    planes,\n","                    planes,\n","                    1,\n","                    stride=1,\n","                    bias=False,\n","                    conv_cfg=self.conv_cfg,\n","                    norm_cfg=self.norm_cfg,\n","                    act_cfg=None))\n","        conv2_kernel_size = self.conv2.conv.kernel_size\n","        conv2_stride = self.conv2.conv.stride\n","        conv2_padding = self.conv2.conv.padding\n","        conv2_dilation = self.conv2.conv.dilation\n","        conv2_bias = bool(self.conv2.conv.bias)\n","        self.conv2 = ConvModule(\n","            planes,\n","            planes,\n","            conv2_kernel_size,\n","            stride=conv2_stride,\n","            padding=conv2_padding,\n","            dilation=conv2_dilation,\n","            bias=conv2_bias,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg,\n","            groups=planes)\n","        conv2.append(self.conv2)\n","        self.conv2 = nn.Sequential(*conv2)\n","\n","\n","class ResNet3dCSN(ResNet3d):\n","    \"\"\"ResNet backbone for CSN.\n","    Args:\n","        depth (int): Depth of ResNetCSN, from {18, 34, 50, 101, 152}.\n","        pretrained (str | None): Name of pretrained model.\n","        temporal_strides (tuple[int]):\n","            Temporal strides of residual blocks of each stage.\n","            Default: (1, 2, 2, 2).\n","        conv1_kernel (tuple[int]): Kernel size of the first conv layer.\n","            Default: (3, 7, 7).\n","        conv1_stride_t (int): Temporal stride of the first conv layer.\n","            Default: 1.\n","        pool1_stride_t (int): Temporal stride of the first pooling layer.\n","            Default: 1.\n","        norm_cfg (dict): Config for norm layers. required keys are `type` and\n","            `requires_grad`.\n","            Default: dict(type='BN3d', requires_grad=True, eps=1e-3).\n","        inflate_style (str): `3x1x1` or `3x3x3`. which determines the kernel\n","            sizes and padding strides for conv1 and conv2 in each block.\n","            Default: '3x3x3'.\n","        bottleneck_mode (str): Determine which ways to factorize a 3D\n","            bottleneck block using channel-separated convolutional networks.\n","                If set to 'ip', it will replace the 3x3x3 conv2 layer with a\n","                1x1x1 traditional convolution and a 3x3x3 depthwise\n","                convolution, i.e., Interaction-preserved channel-separated\n","                bottleneck block.\n","                If set to 'ir', it will replace the 3x3x3 conv2 layer with a\n","                3x3x3 depthwise convolution, which is derived from preserved\n","                bottleneck block by removing the extra 1x1x1 convolution,\n","                i.e., Interaction-reduced channel-separated bottleneck block.\n","            Default: 'ip'.\n","        kwargs (dict, optional): Key arguments for \"make_res_layer\".\n","    \"\"\"\n","\n","    def __init__(self,\n","                 depth,\n","                 pretrained,\n","                 temporal_strides=(1, 2, 2, 2),\n","                 conv1_kernel=(3, 7, 7),\n","                 conv1_stride_t=1,\n","                 pool1_stride_t=1,\n","                 norm_cfg=dict(type='BN3d', requires_grad=True, eps=1e-3),\n","                 inflate_style='3x3x3',\n","                 bottleneck_mode='ir',\n","                 bn_frozen=False,\n","                 **kwargs):\n","        self.arch_settings = {\n","            # 18: (BasicBlock3d, (2, 2, 2, 2)),\n","            # 34: (BasicBlock3d, (3, 4, 6, 3)),\n","            50: (CSNBottleneck3d, (3, 4, 6, 3)),\n","            101: (CSNBottleneck3d, (3, 4, 23, 3)),\n","            152: (CSNBottleneck3d, (3, 8, 36, 3))\n","        }\n","        self.bn_frozen = bn_frozen\n","        if bottleneck_mode not in ['ip', 'ir']:\n","            raise ValueError(f'Bottleneck mode must be \"ip\" or \"ir\",'\n","                             f'but got {bottleneck_mode}.')\n","        super(ResNet3dCSN, self).__init__(\n","            depth,\n","            pretrained,\n","            temporal_strides=temporal_strides,\n","            conv1_kernel=conv1_kernel,\n","            conv1_stride_t=conv1_stride_t,\n","            pool1_stride_t=pool1_stride_t,\n","            norm_cfg=norm_cfg,\n","            inflate_style=inflate_style,\n","            bottleneck_mode=bottleneck_mode,\n","            **kwargs)\n","\n","    def train(self, mode=True):\n","        super(ResNet3d, self).train(mode)\n","        self._freeze_stages()\n","        if mode and self.norm_eval:\n","            for m in self.modules():\n","                if isinstance(m, _BatchNorm):\n","                    m.eval()\n","                    if self.bn_frozen:\n","                        for param in m.parameters():\n","                            param.requires_grad = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.695096Z","iopub.status.busy":"2023-10-15T18:47:57.694709Z","iopub.status.idle":"2023-10-15T18:47:57.714336Z","shell.execute_reply":"2023-10-15T18:47:57.713368Z","shell.execute_reply.started":"2023-10-15T18:47:57.695017Z"},"papermill":{"duration":0.028306,"end_time":"2023-10-11T16:15:35.822670","exception":false,"start_time":"2023-10-11T16:15:35.794364","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def filterMask(mask, thresh):\n","    for i in range(mask.shape[0]):\n","        if np.sum(mask[i]) < thresh:\n","            mask[i] *= 0\n","    return mask\n","\n","def findSingleBoundingBoxPerClass(prediction : np.array, padding : float) -> dict:\n","    bounding_boxes = {}\n","    classes = np.unique(prediction)\n","    z, y, x = prediction.shape\n","    for c in classes:\n","        if c == 0 or c == 5:  # фон и кишечник\n","            continue\n","        mask = (prediction == c).astype(bool)\n","        if c == 1:  # liver\n","            mask = (prediction == c).astype(np.uint8)\n","            mask = filterMask(mask, thresh=500)\n","        coords = np.argwhere(mask)\n","\n","        z_min, y_min, x_min = coords.min(axis=0)\n","        z_max, y_max, x_max = coords.max(axis=0)\n","\n","        x_padding = int((x_max - x_min) * padding)\n","        y_padding = int((y_max - y_min) * padding)\n","        z_padding = int((z_max - z_min) * padding)\n","\n","        x_min = max(x_min - x_padding, 0) / x\n","        y_min = max(y_min - y_padding, 0) / y\n","        z_min = max(z_min - z_padding, 0) / z\n","        \n","        x_max = min(x_max + x_padding, prediction.shape[2]) / x\n","        y_max = min(y_max + y_padding, prediction.shape[1]) / y\n","        z_max = min(z_max + z_padding, prediction.shape[0]) / z\n","        \n","        bounding_boxes[c] = x_min, x_max, y_min, y_max, z_min, z_max\n","    \n","    return bounding_boxes\n","\n","def getCoords(full_cube : np.array, bounding_boxes : dict, label : int, padding : float) -> Tuple[int]:\n","    x_start, x_end, y_start, y_end, z_start, z_end = bounding_boxes[label]\n","    \n","    x_start, x_end = x_start * full_cube.shape[2], x_end * full_cube.shape[2]\n","    x_start, x_end = max(x_start - ((x_end - x_start) * padding), 0) , min(x_end + ((x_end - x_start) * padding), full_cube.shape[2])\n","    \n","    y_start, y_end = y_start * full_cube.shape[1], y_end * full_cube.shape[1]\n","    y_start, y_end = max(y_start - ((y_end - y_start) * padding), 0) , min(y_end + ((y_end - y_start) * padding), full_cube.shape[1])\n","    \n","    z_start, z_end = z_start * full_cube.shape[0], z_end * full_cube.shape[0]\n","    z_start, z_end = max(z_start - ((z_end - z_start) * padding), 0) , min(z_end + ((z_end - z_start) * padding), full_cube.shape[0])\n","    \n","    return int(x_start), int(x_end), int(y_start), int(y_end), int(z_start), int(z_end)\n","\n","def cropOrgan(full_cube : np.array, coords : Tuple[int]) -> np.array:\n","    x_start, x_end, y_start, y_end, z_start, z_end = coords\n","#     print(coords)\n","    \n","    organ = full_cube[z_start : z_end, y_start : y_end, x_start : x_end]\n","    \n","    image_mean = organ.mean()\n","    image_std = organ.std()\n","    organ = (organ - image_mean) / image_std\n","    \n","    return organ\n","\n","def getOrganCube(full_cube : np.array, bounding_boxes : dict, label : int, padding : float) -> np.array:\n","    coords = getCoords(full_cube, bounding_boxes, label, padding)\n","    return cropOrgan(full_cube, coords)\n","\n","def resizeSitk(image, output_size, is_mask=False):\n","    original_size = image.GetSize()\n","    original_spacing = image.GetSpacing()\n","\n","    new_spacing = [\n","        original_spacing[i] * (original_size[i] / output_size[i]) for i in range(3)\n","    ]\n","\n","    resampler = sitk.ResampleImageFilter()\n","    resampler.SetSize(output_size)\n","    resampler.SetOutputSpacing(new_spacing)\n","    resampler.SetTransform(sitk.Transform())\n","    resampler.SetInterpolator(\n","        sitk.sitkNearestNeighbor if is_mask else sitk.sitkLinear\n","    )\n","\n","    resized_image = resampler.Execute(image)\n","\n","    return resized_image\n","\n","def getImageFromArray(image : np.array) -> sitk.Image:\n","    image  = sitk.GetImageFromArray(image)\n","    image.SetOrigin((0, 0, 0))\n","    spacing = image.GetSpacing()\n","    image.SetSpacing(spacing)\n","    return image\n","    \n","def concatKidneys(left : np.array, right : np.array) -> np.array:\n","    z_l, y_l, x_l = left.shape\n","    z_r, y_r, x_r = right.shape\n","    z_max = max(z_l, z_r)\n","    y_max = max(y_l, y_r)\n","    \n","    left_sitk = getImageFromArray(left)\n","    right_sitk = getImageFromArray(right)\n","    \n","    left_resized = resizeSitk(left_sitk, (x_l, y_max, z_max))\n","    right_resized = resizeSitk(right_sitk, (x_r, y_max, z_max))\n","    \n","    left = sitk.GetArrayFromImage(left_resized)\n","    right = sitk.GetArrayFromImage(right_resized)\n","    \n","    kidneys = np.concatenate((left, right), axis = 2)\n","    image_mean = kidneys.mean()\n","    image_std = kidneys.std()\n","    kidneys = (kidneys - image_mean) / image_std\n","    return kidneys\n","\n","def getBestScanId(path_to_scan : List[str]) -> str:\n","    if len(path_to_scan) == 1:\n","        return path_to_scan[0]\n","    if len(os.listdir(path_to_scan[0])) > 200:\n","        return path_to_scan[0] \n","    if len(os.listdir(path_to_scan[1])) > len(os.listdir(path_to_scan[0])):\n","        return path_to_scan[1]\n","    return path_to_scan[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.719308Z","iopub.status.busy":"2023-10-15T18:47:57.718972Z","iopub.status.idle":"2023-10-15T18:47:57.748891Z","shell.execute_reply":"2023-10-15T18:47:57.747760Z","shell.execute_reply.started":"2023-10-15T18:47:57.719285Z"},"trusted":true},"outputs":[],"source":["class DatasetYoloKaggle(Dataset):\n","    def __init__(\n","        self,\n","        cube : np.array,\n","    ):\n","        self.cube = cube\n","       \n","    def __getitem__(self, i):\n","        image = self.cube[i]\n","        if image.shape[0] != 512 or image.shape[1] != 512:\n","            image = cv2.resize(image, (512, 512), interpolation = cv2.INTER_AREA)\n","        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB).T\n","        return image\n","        \n","    def __len__(self):\n","        return self.cube.shape[0]\n","    \n","class ExtravasationPrediction():\n","    def __init__(\n","        self,\n","        path_to_yolo_weights : str,\n","        path_to_pred_models : str,\n","        device : str\n","    ):\n","        self.YOLO = YOLO(path_to_yolo_weights).to(device)\n","        self.prediction_models = self.initializeExtravasationModels(path_to_pred_models)\n","\n","    def initializeExtravasationModels(self, path_to_models : str):\n","        lineag_regressions = []\n","        shifted_lineag_regressions = []\n","        random_forests = []\n","        shifted_random_forests = []\n","        for model_path in glob.glob(os.path.join(path_to_models, \"*.pkl\")):\n","            if model_path.split(\"/\")[-1].split(\"_\")[0] == \"LR\":\n","                if model_path.split(\"/\")[-1].split(\"_\")[1] != \"shifted\":\n","                    LR = joblib.load(model_path)\n","                    lineag_regressions.append(LR)\n","                else:\n","                    LR = joblib.load(model_path)\n","                    shifted_lineag_regressions.append(LR)\n","                \n","        full_train_data = pd.read_csv(\"/kaggle/input/extr-models/data/data.csv\")\n","        shifted_train_data = pd.read_csv(\"/kaggle/input/extr-models/data/shifted_labels.csv\")\n","        feature_cols = ['scores_mean', 'scores_max', 'scores_median', 'scores_std', 'scores_std_not_null',\n","                        'shape', \"feature_1\", 'squares_max', 'squares_median', 'squares_std_not_null']\n","        BRF = BalancedRandomForestClassifier(n_estimators=100,\n","                                     criterion=\"gini\",\n","                                     max_depth=None,\n","                                     min_samples_split=2,\n","                                     min_samples_leaf=1,\n","                                     min_weight_fraction_leaf=0.,\n","                                     max_features='sqrt',\n","                                     max_leaf_nodes=None,\n","                                     min_impurity_decrease=0.,\n","                                     bootstrap=True,\n","                                     oob_score=False,\n","                                     sampling_strategy=\"auto\",\n","                                     replacement=False,\n","                                     random_state=21,\n","                                     verbose=0,\n","                                     warm_start=False,\n","                                     class_weight={0 : 1, 1 : 6},\n","                                     ccp_alpha=0.,\n","                                     max_samples=None\n","                                    )\n","        N_SPLITS = len(full_train_data.fold.unique())\n","        for i in range(N_SPLITS):\n","            X_train = full_train_data[full_train_data.fold != i][feature_cols]\n","            y_train = full_train_data[full_train_data.fold != i][\"label\"]\n","            X_val = full_train_data[full_train_data.fold == i][feature_cols]\n","            y_val = full_train_data[full_train_data.fold == i][\"label\"]\n","            fit_BRF = BRF.fit(X_train, y_train)\n","            random_forests.append(fit_BRF)\n","        fit_BRF = BRF.fit(full_train_data[feature_cols], full_train_data[\"label\"])\n","        random_forests.append(fit_BRF)\n","            \n","        N_SPLITS = len(shifted_train_data.fold.unique())\n","        for i in range(N_SPLITS):\n","            X_train = shifted_train_data[shifted_train_data.fold != i][feature_cols]\n","            y_train = shifted_train_data[shifted_train_data.fold != i][\"label\"]\n","            X_val = shifted_train_data[shifted_train_data.fold == i][feature_cols]\n","            y_val = shifted_train_data[shifted_train_data.fold == i][\"label\"]\n","            fit_BRF = BRF.fit(X_train, y_train)\n","            shifted_random_forests.append(fit_BRF)\n","        fit_BRF = BRF.fit(shifted_train_data[feature_cols], shifted_train_data[\"label\"])\n","        shifted_random_forests.append(fit_BRF)\n","\n","        return (lineag_regressions, random_forests, shifted_lineag_regressions, shifted_random_forests)\n","    \n","    def TTA(self, model, batch : np.array, imgsz : int, conf : float):\n","        shape = batch.shape[0]\n","        confs, squares = np.zeros(shape), np.zeros(shape)\n","        for i in range(4):\n","            batch_rot = torch.rot90(batch, k=i, dims=[2, 3])\n","\n","            preds = model.predict(batch_rot, imgsz=imgsz, conf=conf, verbose=False)\n","            for index, pred in enumerate(preds):\n","                if len(pred.boxes.conf) != 0:\n","                    \"\"\"\n","                    беру только 1й предикт по конфиденсу и площади\n","                    \"\"\"\n","                    pred_confs = pred.boxes.conf.detach().cpu().numpy()[0]\n","                    confs[index] += pred_confs / 4\n","                    pred_xywhn = pred.boxes.xywhn.detach().cpu().numpy()[:, 2:]\n","                    squares[index] += (pred_xywhn[:, 0][0] * pred_xywhn[:, 1][0]) / 4\n","\n","        return confs, squares\n","    \n","    def getYoloScores(self, model, batch : np.array, imgsz : int = 512, conf : float = 0.01, is_tta : bool = False):\n","        if is_tta:\n","            confs, squares = self.TTA(model, batch, imgsz, conf)\n","            return confs, squares\n","        else:\n","            shape = batch.shape[0]\n","            confs, squares = np.zeros(shape), np.zeros(shape)\n","            preds = model.predict(batch, imgsz=imgsz, conf=conf, verbose=False)\n","            for index, pred in enumerate(preds):\n","                if len(pred.boxes.conf) != 0:\n","                    \"\"\"\n","                    беру только 1й предикт по конфиденсу и площади\n","                    \"\"\"\n","                    pred_confs = pred.boxes.conf.detach().cpu().numpy()[0]\n","                    confs[index] += pred_confs\n","                    pred_xywhn = pred.boxes.xywhn.detach().cpu().numpy()[:, 2:]\n","                    squares[index] += pred_xywhn[:, 0][0] * pred_xywhn[:, 1][0]\n","            return confs, squares\n","\n","    def getYoloPredictions(self, cube : np.array, batch_size : int, is_tta : bool = False):\n","        model = self.YOLO\n","        dataset = DatasetYoloKaggle(cube)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","        scores = []\n","        squares = []\n","        for batch in dataloader:\n","            score, square = self.getYoloScores(model, batch, conf=0.1, is_tta=is_tta)\n","            scores.extend(score)\n","            squares.extend(square)\n","        return scores, squares\n","    \n","    def f(self, x, th = 0.35):\n","        counter = 0\n","        m = 0\n","        for i in x:\n","            if i > th:\n","                counter += 1\n","            else:\n","                counter = 0\n","            if counter > m:\n","                m = counter\n","        return m / len(x)\n","\n","    def getFeatures(self, scores : np.array, squares : np.array):\n","        scores = np.array(scores)\n","        squares = np.array(squares)\n","        scores_not_null = scores[scores!=0.0]\n","        scores_mean = scores_not_null.mean()\n","        scores_max = scores.max()\n","        scores_median = np.median(scores_not_null)\n","        scores_std = scores.std()\n","        scores_std_not_null = scores_not_null.std()\n","        shape = len(scores) / 500\n","        my_feature = (self.f(scores, 0.35) * scores_max / (scores_not_null.min() + 1e-6)) * (scores_std_not_null/(scores_std + 1e-6))\n","        squares_max = squares.max()\n","        squares_median = np.median(squares[squares!=0.0])\n","        squares_std_not_null = squares[squares!=0.0].std()\n","        return pd.DataFrame({ 'scores_mean' : scores_mean, 'scores_max' : scores_max, 'scores_median' : scores_median,\n","                 'scores_std' : scores_std, 'scores_std_not_null' : scores_std_not_null, 'shape' : shape,\n","                 'feature_1' : my_feature, 'squares_max' : squares_max, 'squares_median' : squares_median, 'squares_std_not_null' : squares_std_not_null}, index=[0])\n","        \n","\n","    def getExtravasationPrediction(self, cube : np.array, batch_size : int, use_shifted_data : bool = False):\n","        scores, squares = self.getYoloPredictions(cube, batch_size, is_tta=False)\n","        features = self.getFeatures(scores, squares)\n","        \n","        NUM_MODELS = 5\n","        prediction_models = self.prediction_models\n","        linear_regressions = prediction_models[0]\n","        random_forests = prediction_models[1]\n","        shifted_linear_regressions = prediction_models[2]\n","        shifted_random_forests = prediction_models[3]\n","        LR_scores = np.array([0., 0.])\n","        RF_scores = np.array([0., 0.])\n","        for fold_num in range(NUM_MODELS):\n","            LR = linear_regressions[fold_num]\n","            RF = random_forests[fold_num]\n","            LR_pred = LR.predict_proba(features)[0]\n","            RF_pred = RF.predict_proba(features)[0]\n","            LR_scores += LR_pred / NUM_MODELS\n","            RF_scores += RF_pred / NUM_MODELS\n","        score = 0.5 * LR_scores + 0.5 * RF_scores\n","        if use_shifted_data:\n","            LR_scores_shifted = np.array([0., 0.])\n","            RF_scores_shifted = np.array([0., 0.])\n","            for fold_num in range(NUM_MODELS):\n","                LR = shifted_linear_regressions[fold_num]\n","                RF = shifted_random_forests[fold_num]\n","                LR_pred = LR.predict_proba(features)[0]\n","                RF_pred = RF.predict_proba(features)[0]\n","                LR_scores_shifted += LR_pred / NUM_MODELS\n","                RF_scores_shifted += RF_pred / NUM_MODELS\n","            score_shifted = 0.5 * LR_scores_shifted + 0.5 * RF_scores_shifted\n","            score = 0.8 * score + 0.2 * score_shifted\n","        return score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.750547Z","iopub.status.busy":"2023-10-15T18:47:57.750154Z","iopub.status.idle":"2023-10-15T18:47:57.766295Z","shell.execute_reply":"2023-10-15T18:47:57.765301Z","shell.execute_reply.started":"2023-10-15T18:47:57.750510Z"},"papermill":{"duration":0.014379,"end_time":"2023-10-11T16:15:35.884960","exception":false,"start_time":"2023-10-11T16:15:35.870581","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class Config:\n","    #dataset\n","    ROOT = \"data\"\n","    path_to_images   : str = f\"{ROOT}/train_images\"\n","#     path_to_meta_csv : str = f\"{ROOT}/seg_meta.csv\"\n","    path_to_meta_csv : str = f\"{ROOT}/train_series_meta.csv\" # for all images\n","        \n","    path_to_masks    : str = f\"{ROOT}/segmentations_png\"\n","    fold       : int = 0\n","    height     : int = 256\n","    width      : int = 256\n","    slice_size : int = 112\n","    num_workes : int = 6\n","    \n","    #training\n","    num_classes      : int = 6\n","    batch_size       : int = 1\n","    wandb_project    : str = 'RSNA_segmentation'\n","    default_root_dir : str = 'seg'\n","    checkpoints_dir  : str = 'seg/checkpoints'\n","    lr               : float = 1e-3\n","    max_epochs       : int = 120"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.768541Z","iopub.status.busy":"2023-10-15T18:47:57.768150Z","iopub.status.idle":"2023-10-15T18:47:57.795427Z","shell.execute_reply":"2023-10-15T18:47:57.794434Z","shell.execute_reply.started":"2023-10-15T18:47:57.768495Z"},"papermill":{"duration":0.03869,"end_time":"2023-10-11T16:15:35.930507","exception":false,"start_time":"2023-10-11T16:15:35.891817","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import Conv3d\n","\n","# from zoo.resnet3d_csn import ResNet3dCSN\n","\n","encoder_params = {\n","    \"r152ir\": {\n","        \"filters\": [64, 256, 512, 1024, 2048],\n","        \"decoder_filters\": [40, 64, 128, 256],\n","    },\n","    \"r50ir\": {\n","        \"filters\": [64, 256, 512, 1024, 2048],\n","        \"decoder_filters\": [40, 64, 128, 256],\n","    },\n","}\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n","            nn.SiLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.layer(x)\n","\n","\n","class LastDecoderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Upsample(scale_factor=(1, 2, 2)),\n","            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n","            nn.SiLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.layer(x)\n","\n","\n","class ConcatBottleneck(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.seq = nn.Sequential(\n","            nn.Conv3d(in_channels, out_channels, 3, padding=1), nn.SiLU(inplace=True)\n","        )\n","\n","    def forward(self, dec, enc):\n","        x = torch.cat([dec, enc], dim=1)\n","        return self.seq(x)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        decoder_filters,\n","        filters,\n","        upsample_filters=32,\n","        decoder_block=DecoderBlock,\n","        bottleneck=ConcatBottleneck,\n","    ):\n","        super().__init__()\n","        self.decoder_filters = decoder_filters\n","        self.filters = filters\n","        self.decoder_block = decoder_block\n","        self.decoder_stages = nn.ModuleList(\n","            [self._get_decoder(idx) for idx in range(0, len(decoder_filters))]\n","        )\n","        self.bottlenecks = nn.ModuleList(\n","            [\n","                bottleneck(self.filters[-i - 2] + f, f)\n","                for i, f in enumerate(reversed(decoder_filters))\n","            ]\n","        )\n","        self.last_block = None\n","        if upsample_filters:\n","            self.last_block = LastDecoderBlock(\n","                decoder_filters[0], out_channels=upsample_filters\n","            )\n","        else:\n","            self.last_block = nn.Upsample(scale_factor=(1, 2, 2), mode=\"trilinear\")\n","\n","    def forward(self, encoder_results: list):\n","        x = encoder_results[0]\n","        bottlenecks = self.bottlenecks\n","        for idx, bottleneck in enumerate(bottlenecks):\n","            rev_idx = -(idx + 1)\n","            x = self.decoder_stages[rev_idx](x)\n","            x = bottleneck(x, encoder_results[-rev_idx])\n","        if self.last_block:\n","            x = self.last_block(x)\n","        return x\n","\n","    def _get_decoder(self, layer):\n","        idx = layer + 1\n","        if idx == len(self.decoder_filters):\n","            in_channels = self.filters[idx]\n","        else:\n","            in_channels = self.decoder_filters[idx]\n","        return self.decoder_block(in_channels, self.decoder_filters[max(layer, 0)])\n","\n","\n","def _initialize_weights(module):\n","    for m in module.modules():\n","        if (\n","            isinstance(m, nn.Conv2d)\n","            or isinstance(m, nn.Conv3d)\n","            or isinstance(m, nn.ConvTranspose2d)\n","            or isinstance(m, nn.Linear)\n","        ):\n","            m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm3d):\n","            m.weight.data.fill_(1)\n","            m.bias.data.zero_()\n","\n","\n","class Conv2P1DBnAct(nn.Module):\n","    def __init__(\n","        self, in_channels: int, out_channels: int, act=nn.ReLU, bn=False\n","    ) -> None:\n","        super().__init__()\n","        self.conv1_s = nn.Conv3d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=(1, 3, 3),\n","            stride=(1, 1, 1),\n","            padding=(0, 1, 1),\n","            bias=not bn,\n","        )\n","        self.bn1_s = nn.BatchNorm3d(out_channels) if bn else nn.Identity()\n","        self.conv1_t = nn.Conv3d(\n","            out_channels,\n","            out_channels,\n","            kernel_size=(3, 1, 1),\n","            stride=(1, 1, 1),\n","            padding=(1, 0, 0),\n","            bias=not bn,\n","        )\n","        self.bn1_t = nn.BatchNorm3d(out_channels) if bn else nn.Identity()\n","        self.relu = act(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv1_s(x)\n","        x = self.bn1_s(x)\n","        x = self.relu(x)\n","        x = self.conv1_t(x)\n","        x = self.bn1_t(x)\n","        x = self.relu(x)\n","        return x\n","\n","\n","class DecoderBlockConv2P1D(nn.Module):\n","    def __init__(self, in_channels, out_channels, scale_factor=(2, 2, 2)):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Upsample(scale_factor=scale_factor),\n","            Conv2P1DBnAct(in_channels, out_channels, bn=False),\n","        )\n","\n","    def forward(self, x):\n","        return self.layer(x)\n","\n","\n","class ConcatBottleneckConv2P1D(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.seq = nn.Sequential(Conv2P1DBnAct(in_channels, out_channels, bn=False))\n","\n","    def forward(self, dec, enc):\n","        x = torch.cat([dec, enc], dim=1)\n","        return self.seq(x)\n","\n","\n","class CSNDecoder(Decoder):\n","    def _get_decoder(self, layer):\n","        idx = layer + 1\n","        if idx == len(self.decoder_filters):\n","            in_channels = self.filters[idx]\n","        else:\n","            in_channels = self.decoder_filters[idx]\n","        if idx == 1:\n","            return self.decoder_block(\n","                in_channels, self.decoder_filters[max(layer, 0)], (1, 2, 2)\n","            )\n","        else:\n","            return self.decoder_block(in_channels, self.decoder_filters[max(layer, 0)])\n","\n","\n","class ResNet3dCSN2P1D(nn.Module):\n","    def __init__(self, encoder=\"r152ir\", num_classes=1) -> None:\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.decoder = CSNDecoder(\n","            decoder_filters=encoder_params[encoder][\"decoder_filters\"],\n","            filters=encoder_params[encoder][\"filters\"],\n","            decoder_block=DecoderBlockConv2P1D,\n","            bottleneck=ConcatBottleneckConv2P1D,\n","        )\n","        self.final = Conv3d(32, out_channels=num_classes, kernel_size=1)\n","        _initialize_weights(self)\n","\n","        self.backbone = ResNet3dCSN(\n","            pretrained2d=False,\n","            pretrained=None,\n","            depth=int(encoder[1:-2]),\n","            with_pool2=False,\n","            bottleneck_mode=\"ir\",\n","            norm_eval=False,\n","            zero_init_residual=False,\n","        )\n","\n","    def forward(self, x):\n","        x = x.repeat(1, 3, 1, 1, 1)[:, :, :, :, :]\n","        encoder_results = list(reversed(self.backbone(x)))\n","        x = self.decoder(encoder_results)\n","        x = self.final(x)\n","        return {\"mask\": x}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.797474Z","iopub.status.busy":"2023-10-15T18:47:57.796994Z","iopub.status.idle":"2023-10-15T18:47:57.822754Z","shell.execute_reply":"2023-10-15T18:47:57.821723Z","shell.execute_reply.started":"2023-10-15T18:47:57.797442Z"},"papermill":{"duration":0.029731,"end_time":"2023-10-11T16:15:35.966967","exception":false,"start_time":"2023-10-11T16:15:35.937236","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def get_files(folder, ext=''):\n","    paths = []\n","    for item in os.listdir(folder):\n","        item_path = os.path.join(folder, item)\n","        if os.path.isfile(item_path) and item.endswith(ext):\n","            paths.append(item_path)\n","        elif os.path.isdir(item_path):\n","            paths.extend(get_files(item_path, ext))\n","    return paths\n","\n","def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    pixel_array = dcm.pixel_array\n","    if dcm.PixelRepresentation == 1:\n","        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n","        dtype = pixel_array.dtype \n","        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n","    return pixel_array\n","\n","def read_dicom(path: str, fix_monochrome: bool = True) -> np.ndarray:\n","    dicom = pydicom.dcmread(path)\n","    data = standardize_pixel_array(dicom)\n","    \n","    # find rescale params\n","    if (\"RescaleIntercept\" in dicom) and (\"RescaleSlope\" in dicom):\n","        intercept = float(dicom.RescaleIntercept)\n","        slope = float(dicom.RescaleSlope)\n","\n","    # find clipping params\n","    center = int(dicom.WindowCenter)\n","    width = int(dicom.WindowWidth)\n","    \n","    low = center - width / 2\n","    high = center + width / 2\n","\n","    data = (data * slope) + intercept\n","    \n","    data = np.clip(data, low, high)\n","\n","    data = data - np.min(data)\n","    data = data / (np.max(data) + 1e-5)\n","    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n","        data = 1.0 - data\n","    return (data * 255).astype(np.uint8)\n","\n","def get_cube(path : str, koeff : int) -> np.ndarray:\n","    dicom_paths = get_files(path, ext='.dcm')\n","    sorted_dicom_paths = sorted(dicom_paths, key=lambda s: int(s.split(\"/\")[-1].split(\".\")[0]))\n","    cube = []\n","    for i, image_path in enumerate(sorted_dicom_paths):\n","        if i % koeff != 0:\n","            continue\n","        image = read_dicom(image_path)\n","        cube.append(image)\n","    return np.stack(cube)\n","\n","def get_dicom_tags(path : str) -> pd.DataFrame:\n","    dicoms_tags = pd.read_parquet(path)\n","    dicoms_tags[\"z\"]   = dicoms_tags.ImagePositionPatient.apply(lambda x: float(x.split(\", \")[-1][:-1]))\n","    dicoms_tags[\"patient_id\"] = dicoms_tags.path.apply(lambda x: x.split(\"/\")[1])\n","    dicoms_tags[\"series_id\"] = dicoms_tags.path.apply(lambda x: x.split(\"/\")[2])\n","    dicoms_tags[\"slice_id\"] = dicoms_tags.path.apply(lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n","    return dicoms_tags\n","\n","def get_spacing(df : pd.DataFrame, patient_id : str, series_id : str) -> Tuple[float]:\n","    try:\n","        rows = df[(df.patient_id == patient_id) & (df.series_id == series_id)][[\"SliceThickness\", \"z\", \"slice_id\"]][:2]\n","        thikness = rows.SliceThickness[:1].item()\n","\n","        diff = rows.diff()[-1:]\n","\n","        spacing = round((diff.z / ((diff.slice_id) + 1e-6)).item(), 3)\n","        return abs(spacing), thikness\n","    except:\n","        return 2.5, 2.5\n","\n","\n","def resize_sitk(image, output_size : Tuple[int], is_mask : bool = False):\n","    original_size = image.GetSize()\n","    original_spacing = image.GetSpacing()\n","    \n","    new_spacing = [\n","        original_spacing[i] * (original_size[i] / output_size[i]) for i in range(3)\n","    ]\n","\n","    resampler = sitk.ResampleImageFilter()\n","    resampler.SetSize(output_size)\n","    resampler.SetOutputSpacing(new_spacing)\n","    resampler.SetTransform(sitk.Transform())\n","    resampler.SetInterpolator(\n","        sitk.sitkNearestNeighbor if is_mask else sitk.sitkLinear\n","    )\n","\n","    resized_image = resampler.Execute(image)\n","\n","    return resized_image\n","\n","def normalize_image(image : np.array) -> np.array:\n","    image_mean = image.mean()\n","    image_std = image.std()\n","    image = (image - image_mean) / image_std\n","    return image\n","\n","def resize_cube_sitk(image : np.array, width : int, height : int, slice_size : int) -> np.array:\n","    image_sitk = sitk.GetImageFromArray(image)\n","    resized_image_sitk = resize_sitk(\n","            image_sitk,\n","            (width, height, slice_size),\n","            is_mask=False,\n","            )\n","    image = sitk.GetArrayFromImage(resized_image_sitk)\n","    return image\n","\n","def prepare_cube_for_segmentation(image : np.array, width : int, height : int, slice_size : int) -> np.array:\n","    resized_image = resize_cube_sitk(image, width, height, slice_size)\n","    normalized_image = normalize_image(resized_image)\n","    image = np.expand_dims(normalized_image, 0)\n","    sample = {}\n","    sample[\"image\"] = torch.from_numpy(image.astype(np.float32))\n","    return sample\n","\n","def padding3D(image : sitk.Image, width, height, slice_size, full_zeroes=False) -> np.ndarray:\n","    shape = (width, height, slice_size)\n","    if any([image.GetSize()[i] > shape[i] for i in range(3)]):\n","        resize_factor = min([shape[i] / image.GetSize()[i] for i in range(3)])\n","        new_size = [int(image.GetSize()[i] * resize_factor) for i in range(3)]\n","        image = resize_sitk(image, new_size)\n","        \n","    def pad_helper(img_array: np.ndarray, shape: tuple, is_mask=False) -> np.ndarray:\n","        # Determine the value to pad with\n","        initial_value = np.min(img_array) if full_zeroes or is_mask else 0\n","\n","        # Create an empty array with the desired shape\n","        pad_img_array = np.full(shape, initial_value, dtype=img_array.dtype)\n","\n","        # Calculate the starting indices for the input image (to center it)\n","        offset = [(shape[i] - img_array.shape[i]) // 2 for i in range(3)]\n","\n","        # Insert the input image into the empty array\n","        pad_img_array[\n","            offset[0] : offset[0] + img_array.shape[0],\n","            offset[1] : offset[1] + img_array.shape[1],\n","            offset[2] : offset[2] + img_array.shape[2],\n","        ] = img_array\n","        \n","        return pad_img_array\n","    \n","    padded_image = pad_helper(sitk.GetArrayFromImage(image).T, shape)\n","\n","    return padded_image\n","        \n","\n","def getOrganForClassification(image, width, height, slice_size):\n","    image_sitk = sitk.GetImageFromArray(image)\n","    image = padding3D(image_sitk, width, height, slice_size)\n","    image = np.stack([image, image, image])\n","    image = np.expand_dims(image, 0)\n","    sample = {}\n","    sample[\"image\"] = torch.from_numpy(image.astype(np.float32))\n","    return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.824834Z","iopub.status.busy":"2023-10-15T18:47:57.823976Z","iopub.status.idle":"2023-10-15T18:47:57.836349Z","shell.execute_reply":"2023-10-15T18:47:57.835295Z","shell.execute_reply.started":"2023-10-15T18:47:57.824800Z"},"papermill":{"duration":0.01361,"end_time":"2023-10-11T16:15:35.987236","exception":false,"start_time":"2023-10-11T16:15:35.973626","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["TYPE            = \"test\"\n","IMAGES_PATH     = f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/{TYPE}_images\"\n","DICOM_TAGS_PATH = f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/{TYPE}_dicom_tags.parquet\"\n","DEVICE          = \"cuda\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:57.838626Z","iopub.status.busy":"2023-10-15T18:47:57.837610Z","iopub.status.idle":"2023-10-15T18:47:58.787430Z","shell.execute_reply":"2023-10-15T18:47:58.786524Z","shell.execute_reply.started":"2023-10-15T18:47:57.838446Z"},"papermill":{"duration":7.192672,"end_time":"2023-10-11T16:15:43.186446","exception":false,"start_time":"2023-10-11T16:15:35.993774","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["encoder_name = \"r50ir\"\n","modelSegm = ResNet3dCSN2P1D(encoder=encoder_name, num_classes=Config.num_classes).to(torch.float32).to(DEVICE)\n","modelSegm.load_state_dict(torch.load(\"/kaggle/input/rsna-models/seg_fold0.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:58.789598Z","iopub.status.busy":"2023-10-15T18:47:58.788759Z","iopub.status.idle":"2023-10-15T18:47:58.800001Z","shell.execute_reply":"2023-10-15T18:47:58.799095Z","shell.execute_reply.started":"2023-10-15T18:47:58.789564Z"},"papermill":{"duration":0.018821,"end_time":"2023-10-11T16:15:43.212640","exception":false,"start_time":"2023-10-11T16:15:43.193819","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision.models.video.resnet import VideoResNet\n","\n","\n","def _initialize_weights(module):\n","    for m in module.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","            m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif isinstance(m, nn.BatchNorm2d):\n","            m.weight.data.fill_(1)\n","            m.bias.data.zero_()\n","            \n","class ClassifierResNet3dCSN2P1D(nn.Module):\n","    def __init__(\n","        self,\n","        encoder=\"r50ir\",\n","        pool=\"avg\",\n","        norm_eval=False,\n","        num_classes=1,\n","        head_dropout=0.2,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.head = nn.Sequential(\n","            nn.Linear(2048, 1024),\n","            nn.GELU(),\n","            nn.Dropout(head_dropout),\n","            nn.Linear(1024, num_classes),\n","        )\n","        self.avg_pool = (\n","            nn.AdaptiveAvgPool3d((1, 1, 1))\n","            if pool == \"avg\"\n","            else nn.AdaptiveMaxPool3d((1, 1, 1))\n","        )\n","        self.dropout = nn.Dropout(0.5)\n","        _initialize_weights(self)\n","\n","        self.backbone = ResNet3dCSN(\n","            pretrained2d=False,\n","            pretrained=None,\n","            depth=int(encoder[1:-2]),\n","            with_pool2=False,\n","            bottleneck_mode=encoder[-2:],\n","            norm_eval=norm_eval,\n","            zero_init_residual=False,\n","        )\n","\n","    def forward(self, x):\n","        if x.size(1) == 1:\n","            x = x.repeat(1, 3, 1, 1, 1)[:, :, :, :, :]\n","#             print(\"changed\")\n","        x = self.backbone(x)[-1]\n","        x = self.avg_pool(x)\n","        x = self.dropout(x)\n","        x = x.flatten(1)\n","        x = self.head(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:47:58.802013Z","iopub.status.busy":"2023-10-15T18:47:58.801458Z","iopub.status.idle":"2023-10-15T18:48:04.435535Z","shell.execute_reply":"2023-10-15T18:48:04.434592Z","shell.execute_reply.started":"2023-10-15T18:47:58.801983Z"},"trusted":true},"outputs":[],"source":["kidneyModel_1  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","kidneyModel_1.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/kidneys/kidneys_fold1.pt\"))\n","kidneyModel_1 = kidneyModel_1.eval()\n","\n","kidneyModel_2  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","kidneyModel_2.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/kidneys/kidneys_fold2.pt\"))\n","kidneyModel_2 = kidneyModel_2.eval()\n","\n","kidneyModel_4  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","kidneyModel_4.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/kidneys/kidneys_fold4.pt\"))\n","kidneyModel_4 = kidneyModel_4.eval()\n","#kidney (192, 112, 64)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:04.437389Z","iopub.status.busy":"2023-10-15T18:48:04.437035Z","iopub.status.idle":"2023-10-15T18:48:14.808201Z","shell.execute_reply":"2023-10-15T18:48:14.807294Z","shell.execute_reply.started":"2023-10-15T18:48:04.437358Z"},"papermill":{"duration":13.820666,"end_time":"2023-10-11T16:15:57.040024","exception":false,"start_time":"2023-10-11T16:15:43.219358","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["spleenModel_0  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","spleenModel_0.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/spleen_fold0.pt\"))\n","spleenModel_0 = spleenModel_0.eval()\n","\n","spleenModel_1  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","spleenModel_1.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/spleen_fold1.pt\"))\n","spleenModel_1 = spleenModel_1.eval()\n","\n","spleenModel_2  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","spleenModel_2.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/spleen_fold2.pt\"))\n","spleenModel_2 = spleenModel_2.eval()\n","\n","spleenModel_3  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","spleenModel_3.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/spleen_fold3.pt\"))\n","spleenModel_3 = spleenModel_3.eval()\n","\n","spleenModel_4  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","spleenModel_4.load_state_dict(torch.load(\"/kaggle/input/spleen-weights/spleen_fold4.pt\"))\n","spleenModel_4 = spleenModel_4.eval()\n","\n","# with open(\"/kaggle/input/spleen-weights/spleen.json\", \"r\") as f:\n","#     spleenMeta = json.load(f)\n","    \n","#spleen (z, y, x) (64, 128, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:14.809953Z","iopub.status.busy":"2023-10-15T18:48:14.809652Z","iopub.status.idle":"2023-10-15T18:48:22.232278Z","shell.execute_reply":"2023-10-15T18:48:22.231305Z","shell.execute_reply.started":"2023-10-15T18:48:14.809924Z"},"trusted":true},"outputs":[],"source":["liverModel_0  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","liverModel_0.load_state_dict(torch.load(\"/kaggle/input/rsna-models/last_liver_fold0.pt\"))\n","liverModel_0 = liverModel_0.eval()\n","\n","liverModel_2  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","liverModel_2.load_state_dict(torch.load(\"/kaggle/input/rsna-models/liver/liver/liver_fold2.pt\"))\n","liverModel_2 = liverModel_2.eval()\n","liver_koefs_2 = [0.08499399, 0.05209296, 2.82477031]\n","\n","liverModel_3  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","liverModel_3.load_state_dict(torch.load(\"/kaggle/input/rsna-models/liver/liver/liver_fold3.pt\"))\n","liverModel_3 = liverModel_3.eval()\n","liver_koefs_3 = [0.04162182, 0.01810312, 3.08084416]\n","\n","liverModel_4  = ClassifierResNet3dCSN2P1D(num_classes=3, encoder='r152ir', pool=\"max\").to(torch.float32).to(DEVICE)\n","liverModel_4.load_state_dict(torch.load(\"/kaggle/input/rsna-models/liver/liver/liver_fold4.pt\"))\n","liverModel_4 = liverModel_4.eval()\n","liver_koefs_4 = [0.5054786, 0.3725675, 2.10124541]\n","\n","#liver (z, y, x) (64, 224, 224)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:22.234413Z","iopub.status.busy":"2023-10-15T18:48:22.233584Z","iopub.status.idle":"2023-10-15T18:48:25.766099Z","shell.execute_reply":"2023-10-15T18:48:25.765107Z","shell.execute_reply.started":"2023-10-15T18:48:22.234373Z"},"trusted":true},"outputs":[],"source":["ExtrPrediction  = ExtravasationPrediction(path_to_yolo_weights = \"/kaggle/input/extr-models/best_yolo.pt\",\n","                                          path_to_pred_models = \"/kaggle/input/extr-models\",\n","                                          device = DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.768659Z","iopub.status.busy":"2023-10-15T18:48:25.767945Z","iopub.status.idle":"2023-10-15T18:48:25.774254Z","shell.execute_reply":"2023-10-15T18:48:25.772830Z","shell.execute_reply.started":"2023-10-15T18:48:25.768624Z"},"papermill":{"duration":0.020355,"end_time":"2023-10-11T16:15:57.069581","exception":false,"start_time":"2023-10-11T16:15:57.049226","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["constants = np.array([0.974956, 0.121623,\n","                      0.931948, 1.454767,\n","                      0.93764, 0.150843, 0.127324, \n","                      0.893683, 0.339724, 0.117822,\n","                      0.883248, 0.261024, 0.20331, \n","                      1.24976168e+00], dtype=np.float64)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.776440Z","iopub.status.busy":"2023-10-15T18:48:25.775407Z","iopub.status.idle":"2023-10-15T18:48:25.786593Z","shell.execute_reply":"2023-10-15T18:48:25.785684Z","shell.execute_reply.started":"2023-10-15T18:48:25.776407Z"},"trusted":true},"outputs":[],"source":["# import torchvision\n","# classifier2D  = torchvision.models.efficientnet_v2_m()\n","# classifier2D.classifier[1] = torch.nn.Linear(1280, 3)\n","# classifier2D.load_state_dict(torch.load(\"/kaggle/input/rsna-models/effnet2d_state.pth\"))\n","# classifier2D = classifier2D.to(torch.half).to(DEVICE).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.788615Z","iopub.status.busy":"2023-10-15T18:48:25.787952Z","iopub.status.idle":"2023-10-15T18:48:25.797573Z","shell.execute_reply":"2023-10-15T18:48:25.796431Z","shell.execute_reply.started":"2023-10-15T18:48:25.788586Z"},"trusted":true},"outputs":[],"source":["def getTopkScans(image3d, main_axis:int=0, topK:int=5):\n","\n","    image3d = image3d.squeeze(0)\n","    index_axis = 3 if main_axis == 1 else 2\n","    height_sum = torch.count_nonzero(image3d.squeeze(0).mean(0), dim=(0, main_axis))\n","\n","    desc_values = torch.argsort(height_sum, descending=True)[:topK]\n","    images = torch.index_select(image3d, index_axis , desc_values)\n","\n","    return images\n","def get2DInfer(image3d):\n","    indicesHeight = getTopkScans(image3d, main_axis = 1, topK = 5)\n","    indicesHeight = indicesHeight.permute(-1, 0, 1, 2)\n","    return indicesHeight"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.799625Z","iopub.status.busy":"2023-10-15T18:48:25.799284Z","iopub.status.idle":"2023-10-15T18:48:25.858730Z","shell.execute_reply":"2023-10-15T18:48:25.857836Z","shell.execute_reply.started":"2023-10-15T18:48:25.799597Z"},"papermill":{"duration":0.057191,"end_time":"2023-10-11T16:15:57.133879","exception":false,"start_time":"2023-10-11T16:15:57.076688","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["patientsData = []\n","for patient_id in tqdm(sorted(os.listdir(IMAGES_PATH))):\n","    per_patient_list = []\n","    per_patient_list.append(patient_id)\n","    #bowel\n","\n","    for paths in [glob.glob(os.path.join(IMAGES_PATH, patient_id, \"*\"))]:\n","        path = getBestScanId(paths)\n","\n","                        \n","        # для паблик теста, при одном скане получаем NaN\n","        if len(os.listdir(path)) == 1:\n","            per_patient_list.extend(constants[:-1])\n","            continue\n","        if len(os.listdir(path)) <= 400:\n","            koeff = 1\n","        else:\n","            koeff = int(len(os.listdir(path)) // 200)\n","            \n","        try:\n","            cube = get_cube(path, koeff)\n","            segmentation_sample = prepare_cube_for_segmentation(cube, \n","                                                                width=256, \n","                                                                height=256, \n","                                                                slice_size=112)['image'].to(torch.float32).to(DEVICE)\n","#             modelSegm = modelSegm.to(DEVICE)\n","            with torch.no_grad():\n","                pred = modelSegm(segmentation_sample)['mask'].argmax(dim=1).detach().cpu().numpy()[0]\n","#             modelSegm = modelSegm.to('cpu')\n","            torch.cuda.empty_cache()\n","            del segmentation_sample\n","\n","            bounding_boxes = findSingleBoundingBoxPerClass(pred, padding=0.0)\n","        except:\n","            per_patient_list.extend(constants[:-1])\n","            continue\n","        \n","        try:\n","            if 1 in bounding_boxes.keys():\n","                liver = getOrganCube(cube, bounding_boxes, label=1, padding=0.0)\n","                processedLiver = getOrganForClassification(liver, \n","                                                                    width=224, \n","                                                                    height=224, \n","                                                                    slice_size=64)['image'].to(torch.float32).to(DEVICE)\n","                with torch.no_grad():\n","    #                     liverModel_0 = liverModel_0.to(DEVICE)\n","                    predLiver_0 = liverModel_0(processedLiver).softmax(1).cpu().numpy()[0]\n","                    # predLiver_0 *= [0.60987788438797, 0.18113791942596436, 0.20898419618606567]\n","    #                     liverModel_0 = liverModel_0.to('cpu')\n","\n","    #                     liverModel_2 = liverModel_2.to(DEVICE)\n","                    predLiver_2 = liverModel_2(processedLiver).softmax(1).cpu().numpy()[0]\n","                    # predLiver_2 *= [0.0573028102517128, 0.05544815957546234, 0.8872490525245667]\n","    #                     liverModel_2 = liverModel_2.to('cpu')\n","\n","    #                     liverModel_3 = liverModel_3.to(DEVICE)\n","                    predLiver_3 = liverModel_3(processedLiver).softmax(1).cpu().numpy()[0]\n","                    # predLiver_3 *= [0.04373353719711304, 0.04271698370575905, 0.9135494828224182]\n","    #                     liverModel_3 = liverModel_3.to('cpu')\n","\n","    #                     liverModel_4 = liverModel_4.to(DEVICE)\n","                    predLiver_4 = liverModel_4(processedLiver).softmax(1).cpu().numpy()[0]\n","                    # predLiver_4 *= [0.14689353108406067, 0.12861159443855286, 0.7244949340820312]\n","    #                     liverModel_4 = liverModel_4.to('cpu')\n","\n","                predLiver = 0.1 * predLiver_0 + 0.2 * predLiver_2 + 0.3 * predLiver_3 + 0.4 * predLiver_4\n","\n","\n","                torch.cuda.empty_cache()\n","                del processedLiver\n","            else:\n","                predLiver = [0.893683, 0.339724, 0.117822]\n","        except:\n","            predLiver = [0.893683, 0.339724, 0.117822]\n","\n","            \n","        try:\n","            if 2 in bounding_boxes.keys():\n","                spleen = getOrganCube(cube, bounding_boxes, label=2, padding=0.0)\n","                processedSpleen = getOrganForClassification(spleen, \n","                                                                    width=128, \n","                                                                    height=128, \n","                                                                    slice_size=64)['image'].to(torch.float32).to(DEVICE)\n","    #                 slices_2d = get2DInfer(processedSpleen).to(\"cuda\")\n","    #                 with torch.no_grad():\n","    #                     predSpleen = classifier2D(slices_2d).softmax(1).mean(0).cpu().tolist()\n","\n","\n","    #                 del slices_2d\n","                with torch.no_grad():\n","    #                     spleenModel_0 = spleenModel_0.to(DEVICE)\n","                    predSpleen_0 = spleenModel_0(processedSpleen).softmax(1).cpu().numpy()[0]\n","    #                     spleenModel_0 = spleenModel_0.to('cpu')\n","\n","    #                     spleenModel_1 = spleenModel_1.to(DEVICE)\n","                    predSpleen_1 = spleenModel_1(processedSpleen).softmax(1).cpu().numpy()[0]\n","    #                     spleenModel_1 = spleenModel_1.to('cpu')\n","\n","    #                     spleenModel_2 = spleenModel_2.to(DEVICE)\n","                    predSpleen_2 = spleenModel_2(processedSpleen).softmax(1).cpu().numpy()[0]\n","    #                     spleenModel_2 = spleenModel_2.to('cpu')\n","    \n","    #                     spleenModel_3 = spleenModel_3.to(DEVICE)\n","                    predSpleen_3 = spleenModel_3(processedSpleen).softmax(1).cpu().numpy()[0]\n","    #                     spleenModel_3 = spleenModel_3.to('cpu')\n","\n","    #                     spleenModel_4 = spleenModel_4.to(DEVICE)\n","                    predSpleen_4 = spleenModel_4(processedSpleen).softmax(1).cpu().numpy()[0]\n","    #                     spleenModel_4 = spleenModel_4.to('cpu')\n","    \n","                predSpleen = (predSpleen_0 + predSpleen_1 + predSpleen_2 + predSpleen_3 + predSpleen_4) / 5\n","\n","                # predSpleen *= [4.386, 3.448, 2.336]\n","\n","                torch.cuda.empty_cache()\n","                del processedSpleen\n","            else:\n","                predSpleen = [0.883248, 0.261024, 0.20331]\n","        except:\n","            predSpleen = [0.883248, 0.261024, 0.20331]\n","            \n","        try:\n","            if (3 in bounding_boxes.keys()) and (4 in bounding_boxes.keys()):\n","                right_kidney = getOrganCube(cube, bounding_boxes, label=3, padding=0.0)\n","                left_kidney = getOrganCube(cube, bounding_boxes, label=4, padding=0.0)\n","                kidneys = concatKidneys(left_kidney, right_kidney)\n","                processedKidneys = getOrganForClassification(kidneys, \n","                                                                    width=192, \n","                                                                    height=112, \n","                                                                    slice_size=64)['image'].to(torch.float32).to(DEVICE)\n","    #                 kidneyModel = kidneyModel.to(DEVICE)\n","                with torch.no_grad():\n","    #                     kidneyModel_1 = kidneyModel_1.to(DEVICE)\n","                    predKidneys_1 = kidneyModel_1(processedKidneys).softmax(1).cpu().numpy()[0]\n","    #                     kidneyModel_1 = kidneyModel_1.to('cpu')\n","\n","    #                     kidneyModel_2 = kidneyModel_2.to(DEVICE)\n","                    predKidneys_2 = kidneyModel_2(processedKidneys).softmax(1).cpu().numpy()[0]\n","    #                     kidneyModel_2 = kidneyModel_2.to('cpu')\n","    \n","#                     kidneyModel_2 = kidneyModel_2.to(DEVICE)\n","                    predKidneys_4 = kidneyModel_4(processedKidneys).softmax(1).cpu().numpy()[0]\n","#                     kidneyModel_2 = kidneyModel_2.to('cpu')\n","\n","                predKidneys = 0.3 * predKidneys_1 + 0.3 * predKidneys_2 + 0.4 * predKidneys_4\n","\n","                # predKidneys *= [4.45, 2.38, 1.46]\n","\n","                torch.cuda.empty_cache()\n","                del processedKidneys\n","            else:\n","                predKidneys = [0.93764, 0.150843, 0.127324]\n","        except:\n","            predKidneys = [0.93764, 0.150843, 0.127324]\n","\n","        pred_extravasation = ExtrPrediction.getExtravasationPrediction(cube, batch_size=50)\n","        # pred_extravasation *= [2.25, 1]\n","#         pred_extravasation = [0.931948, 1.454767]\n","        \n","        #bowel   \n","        per_patient_list.append(constants[0])\n","        per_patient_list.append(constants[1])\n","        #extravasion\n","        per_patient_list.extend(pred_extravasation)  \n","        \n","        #три значения для органов в порядке из датафрейма\n","#         predKidneys = [0.93764, 0.150843, 0.127324]\n","        per_patient_list.extend(predKidneys)\n","        \n","#         predLiver = [0.893683, 0.339724, 0.117822]\n","        per_patient_list.extend(predLiver)\n","        \n","#         predSpleen = [0.883248, 0.261024, 0.20331]\n","        per_patient_list.extend(predSpleen)\n","        #any_injury\n","#         per_patient_list.append(constants[-1])\n","#     print(per_patient_list)\n","    patientsData.append(per_patient_list)\n","        # для последующего ресайза органов и подготовки их к классификации можно использовать функцию prepare_cube_for_segmentation с соотв. параметрами\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.860565Z","iopub.status.busy":"2023-10-15T18:48:25.859833Z","iopub.status.idle":"2023-10-15T18:48:25.865872Z","shell.execute_reply":"2023-10-15T18:48:25.864996Z","shell.execute_reply.started":"2023-10-15T18:48:25.860533Z"},"papermill":{"duration":0.014531,"end_time":"2023-10-11T16:15:57.155807","exception":false,"start_time":"2023-10-11T16:15:57.141276","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# columns = list(columns)\n","# columns.append('any_injury')\n","# submission = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/sample_submission.csv')\n","submission = pd.DataFrame(patientsData, columns=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.867859Z","iopub.status.busy":"2023-10-15T18:48:25.867267Z","iopub.status.idle":"2023-10-15T18:48:25.877194Z","shell.execute_reply":"2023-10-15T18:48:25.876305Z","shell.execute_reply.started":"2023-10-15T18:48:25.867828Z"},"papermill":{"duration":0.01321,"end_time":"2023-10-11T16:15:57.175912","exception":false,"start_time":"2023-10-11T16:15:57.162702","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# def check_injury(series)->bool:\n","#     return series.bowel_healthy > series.bowel_healthy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.879106Z","iopub.status.busy":"2023-10-15T18:48:25.878475Z","iopub.status.idle":"2023-10-15T18:48:25.903079Z","shell.execute_reply":"2023-10-15T18:48:25.902090Z","shell.execute_reply.started":"2023-10-15T18:48:25.879077Z"},"papermill":{"duration":0.027786,"end_time":"2023-10-11T16:15:57.210549","exception":false,"start_time":"2023-10-11T16:15:57.182763","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T18:48:25.908003Z","iopub.status.busy":"2023-10-15T18:48:25.907383Z","iopub.status.idle":"2023-10-15T18:48:25.915498Z","shell.execute_reply":"2023-10-15T18:48:25.914589Z","shell.execute_reply.started":"2023-10-15T18:48:25.907980Z"},"papermill":{"duration":0.016596,"end_time":"2023-10-11T16:15:57.234416","exception":false,"start_time":"2023-10-11T16:15:57.217820","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.007028,"end_time":"2023-10-11T16:15:57.248828","exception":false,"start_time":"2023-10-11T16:15:57.241800","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
